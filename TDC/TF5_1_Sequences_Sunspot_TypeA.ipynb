{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF5_1.Sequence_Sunspot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6km7HWwRkiA"
      },
      "source": [
        "# Build and train a neural network to predict sunspot activity using the Sunspots.csv dataset. \n",
        "# Your neural network must have an MAE of 0.12 or less on the normalized dataset for top marks. \n",
        "# Code for normalizing the data is provided and should not be changed. \n",
        "# \n",
        "# At the bottom of this file, we provide some testing code in case you want to check your model. \n",
        "# Note: Do not use lambda layers in your model, they are not supported on the grading infrastructure. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv0xJol8R8K8"
      },
      "source": [
        "import csv \n",
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import urllib "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY2wkSwAR-ss"
      },
      "source": [
        "# DO NOT CHANGE THIS CODE \n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer): \n",
        "    series = tf.expand_dims(series, axis=-1) \n",
        "    ds = tf.data.Dataset.from_tensor_slices(series) \n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True) \n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1)) \n",
        "    ds = ds.shuffle(shuffle_buffer) \n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:])) \n",
        "    return ds.batch(batch_size).prefetch(1) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-pc3jP_SKeu"
      },
      "source": [
        "def solution_model(): \n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv' \n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv') \n",
        "    \n",
        "    time_step = [] \n",
        "    sunspots = [] \n",
        "\n",
        "    with open('sunspots.csv') as csvfile: \n",
        "        reader = csv.reader(csvfile, delimiter=',') \n",
        "        next(reader) \n",
        "        for row in reader: \n",
        "            sunspots.append(float(row[2])) \n",
        "            time_step.append(int(row[0])) \n",
        "\n",
        "    series = np.array(sunspots) # YOUR CODE HERE \n",
        "\n",
        "    # DO NOT CHANGE THIS CODE \n",
        "    # This is the normalization function \n",
        "    min = np.min(series) \n",
        "    max = np.max(series) \n",
        "    series -= min \n",
        "    series /= max \n",
        "    time = np.array(time_step) \n",
        "    \n",
        "    # The data should be split into training and validation sets at time step 3000 \n",
        "    # DO NOT CHANGE THIS CODE \n",
        "    split_time = 3000 \n",
        "    time_train = time[:split_time] # YOUR CODE HERE \n",
        "    x_train = series[:split_time] # YOUR CODE HERE \n",
        "    time_valid = time[split_time:] # YOUR CODE HERE \n",
        "    x_valid = series[split_time:] # YOUR CODE HERE \n",
        "\n",
        "    # DO NOT CHANGE THIS CODE \n",
        "    window_size = 30 \n",
        "    batch_size = 32 \n",
        "    shuffle_buffer_size = 1000 \n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size) \n",
        "    model = tf.keras.models.Sequential([ \n",
        "        # YOUR CODE HERE. \n",
        "        # Whatever your first layer is, the input shape will be [None,1] when using the Windowed_dataset above, \n",
        "        # depending on the layer type chosen \n",
        "        tf.keras.layers.Conv1D(filters=60, kernel_size=10, \n",
        "                                strides=1, padding=\"causal\", \n",
        "                                activation=\"relu\", input_shape=[None, 1]), \n",
        "        tf.keras.layers.LSTM(60, return_sequences=True), \n",
        "        tf.keras.layers.LSTM(60, return_sequences=True), \n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"), \n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"), \n",
        "        tf.keras.layers.Dense(1) \n",
        "        ]) \n",
        "\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE \n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence \n",
        "    # [[{{node IteratorGetNext}}]] \n",
        "    # \n",
        "\n",
        "    model.compile(loss=tf.keras.losses.Huber(), \n",
        "                  optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9), \n",
        "                    metrics=['mae']) \n",
        "    model.fit(train_set, epochs=50) \n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL \n",
        "\n",
        "    # THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL \n",
        "    # BEFORE UPLOADING YOU CAN DO IT WITH THIS \n",
        "\n",
        "    import math\n",
        "    def model_forecast(model, series, window_size): \n",
        "        ds = tf.data.Dataset.from_tensor_slices(series) \n",
        "        ds = ds.window(window_size, shift=1, drop_remainder=True) \n",
        "        ds = ds.flat_map(lambda w: w.batch(window_size)) \n",
        "        ds = ds.batch(32).prefetch(1) \n",
        "        forecast = model.predict(ds) \n",
        "        return forecast \n",
        "\n",
        "    window_size = 30 # YOUR CODE HERE \n",
        "    rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) \n",
        "    rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0] \n",
        "    result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy() \n",
        "\n",
        "    ## To get the maximum score, your model must have an MAE OF .12 or less. \n",
        "    ## When you Submit and Test your model, the grading infrastructure \n",
        "    ## converts the MAE of your model to a score from 0 to 5 as follows: \n",
        "\n",
        "    test_val = 100 * result \n",
        "    score = math.ceil(17 - test_val) \n",
        "    if score > 5: \n",
        "        score = 5 \n",
        "\n",
        "    print(score)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UuM_4S1SfiE"
      },
      "source": [
        "# Note that you'll need to save your model as a .h5 like this. \n",
        "# When you press the Submit and Test button, your saved .h5 model will \n",
        "# be sent to the testing infrastructure for scoring \n",
        "# and the score will be returned to you. \n",
        "if __name__ == '__main__': \n",
        "    model = solution_model() \n",
        "    model.save(\"mymodel.h5\") \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}