{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_7_RNN_수정.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq",
        "colab_type": "text"
      },
      "source": [
        "# 감성분석 실습\n",
        "\n",
        "<b>학습 목표:    \n",
        "- 한국어 자연어처리의 전반적인 FLOW를 이해한다.\n",
        "- keras.Sequantial 모듈을 이용해 간단한 감성분석 모델을 구현해 학습하고, 학습 결과를 진단한다.</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wg2UE1UiAne",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<font color = \"red\"> \n",
        "QUIZ:   \n",
        "숫자만 인식할 수 있는 기계학습 모델에게 자연어를 인식시키는 방법은? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ695haKKAwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try: \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "\"\"\" 한국어 형태소 분석 라이브러리 \"\"\"\n",
        "!pip install konlpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS",
        "colab_type": "text"
      },
      "source": [
        "# # 1. 자연어처리 플로우 이해하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfgQ96FEjKz",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Parsing\n",
        "- konply : 한국어 자연어처리 관련 패키지\n",
        "- konply의 Okt tagger을 이용해 형태소 분석 실행\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_2.PNG?raw=true\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXHdDffgaggl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "def tokenize(lines): \n",
        "  return [pos[0] for pos in okt.pos(lines)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = \"시간 가는 줄 알고 봤습니다.\"\n",
        "sentence2 = \"안보면 후회ㅠㅠ...\"\n",
        "parsed_sent1 = tokenize(sentence1)\n",
        "parsed_sent2 = tokenize(sentence2)\n",
        "print(\"문장 1:\", parsed_sent1)\n",
        "print(\"문장 2:\", parsed_sent2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdzQGbXMFJNu",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. 모델 인풋 만들기\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_3.PNG?raw=true\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSdN7ROFdwN",
        "colab_type": "text"
      },
      "source": [
        "#### 2-1) 단어 사전 만들기\n",
        "자연어 형태소를 모델이 처리할 수 있는 정수 인덱스로 변환해야 함\n",
        "- 형태소 분석된 단어를 정수로 매핑하는 사전 만들기\n",
        "- 배치 연산을 위해 필요한 Padding([PAD])과 Out of vocabulary([OOV]) 토큰을 항상 맨 앞에 추가해둠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for word in parsed_sent1:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "for word in parsed_sent2:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "print(\"Vocab Dictionary Example:\")\n",
        "print(vocab_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCyqBSGYFjXx",
        "colab_type": "text"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n",
        "- 위에서 만든 vocab_dict를 이용해 파싱해둔 문장을 모델에 태울 수 있는 정수 인덱스로 바꾸기\n",
        "- 기본적으로 LSTM은 가변적인 문장 길이를 인풋으로 받을 수 있지만, 배치 처리를 위해 <font color=\"blue\">max_seq_len</font>을 정해두고 길이를 통일함    \n",
        "    - max_seq_len 보다 짧은 문장에는 max_seq_len이 될 때까지 [PAD]에 해당하는 인덱스를 붙여줌\n",
        "    - max_seq_len 보다 긴 문장은 max_seq_len 개의 토큰만 남기고 자름   \n",
        "       - 앞에서부터 max_seq_len 만큼의 토큰만 사용한다거나\n",
        "       - 뒤에서부터 max_seq_len 만큼의 토큰만 사용하거나\n",
        "       - 중간부분에서 max_seq_len 만큼만 사용함\n",
        "    - tensorflow.keras.preprocessing.sequence의 <font color=\"blue\">pad_sequences</font> 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IfdgEZOEh3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = 10\n",
        "\n",
        "input_id1 = [vocab_dict[word] for word in parsed_sent1]\n",
        "input_id2 = [vocab_dict[word] for word in parsed_sent2]\n",
        "\n",
        "# Padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "input_ids = [input_id1, input_id2]\n",
        "input_ids = pad_sequences(input_ids, maxlen=max_seq_len, value = vocab_dict['[PAD]']) \n",
        "print(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyE471IGDs1",
        "colab_type": "text"
      },
      "source": [
        "### Step3. 모델 만들기\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_4.PNG?raw=true\">\n",
        "\n",
        "- <b>tf.keras.Sequential()</b> 을 사용해 모델 구현하기\n",
        "- Sequential()은 레이어를 연속적으로 쌓아서 모델로 만들 수 있음\n",
        "    - 임베딩 레이어 : layers.Embedding()\n",
        "    - LSTM : layers.LSTM()\n",
        "    - FC layer : layers.Dense()   \n",
        "- LSTM을 사용해 문장을 인코딩하고, Fully Connected layer을 두 층 쌓아 최종 output을 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMgZkCRJSaF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab_dict) # 단어사전 개수\n",
        "embedding_dim = 30 # 임베딩 차원\n",
        "lstm_hidden_dim = 50 # LSTM hidden_size \n",
        "dense_dim = 50 #FC layer size\n",
        "batch_size = 2 # batch size\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.LSTM(lstm_hidden_dim),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiAfCsoSS7nx",
        "colab_type": "text"
      },
      "source": [
        "- <b>model.summary()</b> : 모델 구조, 파라메터 개수를 한 눈에 보여줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbFyrdpWEhyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXeJUihwTHl_",
        "colab_type": "text"
      },
      "source": [
        "- <b>tf.keras.utils.plot_model()</b> : 인풋 ~ 아웃풋까지 텐서의 흐름을 그림으로 나타냄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grdZX_qvSHdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(model, \"LSTM_sentiment_analysis.png\", show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn2t4PeDRjfI",
        "colab_type": "text"
      },
      "source": [
        "- <b>model.predict()</b> 메서드를 사용하면 인풋에 대해 모델의 예측값을 얻을 수 있음.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnagVZncO9U-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.predict(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxTyxC57HY4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, s in enumerate(scores):\n",
        "    print(\"문장 {} → 긍정: {:.2f} / 부정: {:.2f}\".format(i, s[0],s[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btopfDwJv2l3",
        "colab_type": "text"
      },
      "source": [
        "# # 2. LSTM으로 감성분석 모델 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJfJrzDPG1H",
        "colab_type": "text"
      },
      "source": [
        "### Step 0. 학습 데이터 준비하기\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_5.PNG?raw=true\">    \n",
        "\n",
        "- 네이버 영화 감성분석 데이터셋 활용\n",
        "- 훈련 데이터 150,000건, 테스트 데이터 50,000건"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHygGkDeiyUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 네이버 영화 리뷰 데이터셋 다운로드 \"\"\"\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUMj2VCv_Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "    raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "    raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "    FULL_TRAIN.append([line[0], line[1], int(line[2].strip())])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "    FULL_TEST.append([line[0], line[1], int(line[2].strip())])\n",
        "print(\"FULL_TRAIN: {}개 (긍정 {}, 부정 {})\".format(len(FULL_TRAIN), sum([t[2] for t in FULL_TRAIN]), len(FULL_TRAIN)-sum([t[2] for t in FULL_TRAIN])), FULL_TRAIN[0])\n",
        "print(\"FULL_TEST : {}개 (긍정 {}, 부정 {})\".format(len(FULL_TEST), sum([t[2] for t in FULL_TEST]), len(FULL_TEST)-sum([t[2] for t in FULL_TEST])), FULL_TEST[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IWVSphmZ42",
        "colab_type": "text"
      },
      "source": [
        "### label \n",
        "> 0: 부정\n",
        "\n",
        "> 1: 긍정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7II5hOLsEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 예시 : id, 문장, 라벨 순서\n",
        "print(FULL_TRAIN[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyE6Ia5uopA",
        "colab_type": "text"
      },
      "source": [
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_6.PNG?raw=true\">  \n",
        "- 시간 관계상 train 중 50,000건을 학습데이터, 10,000건을 검증 데이터로 사용\n",
        "- test 중 10,000건만 샘플링하여 최종 성능 테스트에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_xG06TPI9vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "print(\"train     : {}개 (긍정 {}, 부정 {})\".format(len(train), sum([t[2] for t in train]), len(train)-sum([t[2] for t in train])), train[0])\n",
        "print(\"validation: {}개 (긍정 {}, 부정 {})\".format(len(val), sum([t[2] for t in val]), len(val)-sum([t[2] for t in val])), val[0])\n",
        "print(\"test      : {}개 (긍정 {}, 부정 {})\".format(len(test), sum([t[2] for t in test]), len(test)-sum([t[2] for t in test])), test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jykZkziwnd2",
        "colab_type": "text"
      },
      "source": [
        "## Step 1. Parsing\n",
        "- Train/ Test의 문장을 형태소분석기로 파싱하여 train_sentences, test_sentences에 저장해둠.\n",
        "- categorical_crossentropy loss를 사용하기 위해 정답 라벨은 One-hot encoding 형식으로 저장\n",
        "   - 부정 -> [1, 0]\n",
        "   - 긍정 -> [0 , 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9HFloKROOnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences = []\n",
        "val_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "# 추후 학습/ 테스트를 위해 라벨 정보 저장해둠\n",
        "train_label_ids = []\n",
        "val_label_ids = []\n",
        "test_label_ids = []\n",
        "\n",
        "print(\"start tokenizing TRAIN sentences\")\n",
        "for i, line in enumerate(train):\n",
        "    tokens = tokenize(line[1])\n",
        "    train_sentences.append(tokens)\n",
        "    if line[2] == 0: # 부정\n",
        "      train_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      train_label_ids.append([0,1])\n",
        "\n",
        "    if (i+1) % 5000 == 0: print(\"... {}/{} done\".format(i+1, len(train)))\n",
        "\n",
        "print(\"example:\", train_sentences[-1], train_label_ids[-1], \"\\n\")\n",
        "\n",
        "print(\"start tokenizing VALIDATION sentences\")\n",
        "\n",
        "for line in val:\n",
        "    tokens = tokenize(line[1])\n",
        "    val_sentences.append(tokens)\n",
        "    if line[2] == 0: # 부정\n",
        "      val_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      val_label_ids.append([0,1])\n",
        "print(\"... done\\n\")\n",
        "\n",
        "print(\"start tokenizing TEST sentences\")\n",
        "for line in test:\n",
        "    tokens = tokenize(line[1])\n",
        "    test_sentences.append(tokens)\n",
        "    if line[2] == 0: # 부정\n",
        "      test_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      test_label_ids.append([0,1])\n",
        "\n",
        "print(\"... done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bDXwmsH2x6",
        "colab_type": "text"
      },
      "source": [
        "##Step 2. 모델 인풋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFXWtJDWdH1E",
        "colab_type": "text"
      },
      "source": [
        "#### 2-1) 단어사전 만들기\n",
        "- 훈련 데이터 문장에 있는 형태소를 이용해 구축\n",
        "- (일반적으로는 더 많은 코퍼스에 대해 구축된 사전을 사용하지만, 편의상 훈련셋만으로 진행)\n",
        "\n",
        "# 실습 MISSION #14\n",
        "[CODE] 부분을 채워넣어 단어사전을 만들고 생성된 단어사전의 크기를 확인해보세요. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAfNMCpgwdww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_dict.keys():\n",
        "            ## [CODE] ##\n",
        "            \n",
        "            ############\n",
        "            i += 1\n",
        "print(\"Vocab Dictionary Size:\", len(vocab_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu_DabQjQYFK",
        "colab_type": "text"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n",
        "\n",
        "# 실습 MISSION #15\n",
        "> 토큰화된 문장들 (tokenized_sentences)을 인풋으로 받아 다음을 처리하는 함수를 만드시오\n",
        "\n",
        "* 단어사전에 없는 단어는 [OOV] 인덱스로 처리하기   \n",
        "* 단어사전에서 매핑되는 단어는 해당 인덱스로 바꾸기   \n",
        "* 문장 길이를 'max_seq_len'으로 맞추어, max_seq_len보다 긴 문장은 뒷부분을 자르고, max_seq_len보다 짧은 문장은 뒷부분에 padding하기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHI2uFCUojkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_input_ids(tokenized_sentences, max_seq_len = 50):\n",
        "  \n",
        "  num_oov = 0 # OOV 발생 개수를 셈\n",
        "  result_input_ids = [] # result_input_ids : 정수 인덱스로 변환한 문장들의 리스트\n",
        "\n",
        "  for sentence in tokenized_sentences :\n",
        "      \"\"\" vocab_dict를 사용해 정수로 변환 \"\"\" \n",
        "      input_ids = []\n",
        "      for token in sentence:\n",
        "          if token not in vocab_dict: \n",
        "              input_ids.append(__________________) ## a. [CODE] OOV 처리\n",
        "              num_oov += 1\n",
        "          else:\n",
        "              input_ids.append(__________________) ## b. [CODE] 단어사전에서 토큰 찾아서 붙이기\n",
        "      \n",
        "      result_input_ids.append(input_ids)\n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  result_input_ids = pad_sequences(result_input_ids, maxlen=____, padding='___', truncating='___', value = 0) ## c. [CODE] padding 하기\n",
        "\n",
        "  return result_input_ids, num_oov\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irm8oGg3ICWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_sentences 처리\n",
        "train_input_ids, num_oov = make_input_ids(train_sentences)\n",
        "\n",
        "print(\"---- TRAIN ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFz1GGuxhGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_sentences 처리\n",
        "val_input_ids, num_oov = make_input_ids(val_sentences)\n",
        "\n",
        "print(\"---- VALIDATION ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYs0YXRjIDg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_sentences 처리\n",
        "test_input_ids, num_oov = make_input_ids(test_sentences)\n",
        "\n",
        "print(\"---- TEST ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7l98rF7c-Er",
        "colab_type": "text"
      },
      "source": [
        "#### 2-3) 라벨 리스트를 np.array로 변환\n",
        "- TIP: tensorflow2.0에서는 numpy array를 인풋으로 받아들임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuFQFHbj3AQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label_ids = np.array(train_label_ids)\n",
        "val_label_ids = np.array(val_label_ids)\n",
        "test_label_ids = np.array(test_label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhcHsul1IHxj",
        "colab_type": "text"
      },
      "source": [
        "## Step3. 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iPCdwGTjzk",
        "colab_type": "text"
      },
      "source": [
        "# 실습 MISSION #16\n",
        "> 아래 조건에 맞는 모델을 만드시오\n",
        " \n",
        "* embedding 차원은 150\n",
        "* LSTM hidden size는 100\n",
        "* Dense의 hidden size는 100, relu activation 사용\n",
        "* output Dense layer에서는 긍/부정 2개 카테고리를 분류하되 softmax 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcDr8GlZTqqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "vocab_size = len(vocab_dict) \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "            ####### MISSION 작성 ######\n",
        "\n",
        "            ###########################\n",
        "])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlpgWelVIMXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GXrD_dTdTa_",
        "colab_type": "text"
      },
      "source": [
        "## Step 4. 모델 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPp_wZ2ddaMG",
        "colab_type": "text"
      },
      "source": [
        "#### 4-1) <b>model.compile()</b>을 통해 loss, optimizer 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nPJiGW1q92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wQWmz8Xdi89",
        "colab_type": "text"
      },
      "source": [
        "#### 4-2) model.fit()을 통해 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6jyqi5cdjJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(train_input_ids, train_label_ids, epochs=num_epochs, validation_data=(val_input_ids, val_label_ids), verbose=2) \n",
        "\n",
        "test_result = model.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpO7-KgJxFZI",
        "colab_type": "text"
      },
      "source": [
        "<font color='purple'>🚴‍♀️<i> while training...</i></font>   \n",
        "<br>\n",
        "<u> keras RNN API 확인하기</u>\n",
        "- https://www.tensorflow.org/guide/keras/rnn\n",
        "- 기본적인 RNN 이외에 Bidirectional RNN, Multi-layer RNN 구조 등을 활용하고 싶다면 API 문서를 참고해 만들 수 있음.\n",
        "- 예) \n",
        "  - LSTM의 모든 timestep의 output을 받아오려면 \n",
        "  - lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)로 설정\n",
        "  - Bidirectional-LSTM을 사용하려면\n",
        "  - layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MnPFs9O3Fr",
        "colab_type": "text"
      },
      "source": [
        "#### 4-3) 훈련 결과 진단하기\n",
        "<font color=\"red\">QUIZ :   \n",
        "a. 현재 모델에 문제점이 있나요?   \n",
        "b. 문제가 나타나고 있다면 이에 대한 해결 방안을 제시해 보세요. \n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtaGxpJa4SmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13AeKZe8JdIn",
        "colab_type": "text"
      },
      "source": [
        "## Step 5. Inference 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxopYLQtIkw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 훈련된 모델로 다시 예측해보기 \"\"\"\n",
        "\n",
        "def inference(mymodel, sentence):\n",
        "  # 1. tokenizer로 문장 파싱\n",
        "  parsed_sent = tokenize(sentence)\n",
        "  input_id = []\n",
        "\n",
        "  # 2. vocab_dict를 이용해 인덱스로 변환\n",
        "  for word in parsed_sent:\n",
        "    if word in vocab_dict: input_id.append(vocab_dict[word])\n",
        "    else: input_id.append(vocab_dict[\"[OOV]\"])\n",
        "  \n",
        "  # 단일 문장 추론이기 때문에 패딩할 필요가 없음 \n",
        "  score = mymodel.predict(np.array([input_id])) \n",
        "\n",
        "  print(\"** INPUT:\", sentence)\n",
        "  print(\"   -> 부정: {:.2f} / 긍정: {:.2f}\".format(score[0][0],score[0][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm172c7-YmBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = \"시간 가는 줄 알고 봤습니다.\"\n",
        "sentence2 = \"안보면 후회ㅠㅠ...\"\n",
        "inference(model, sentence1)\n",
        "inference(model, sentence2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_fIiVC4k8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 원하는 문장에 대해 추론해 보세요\n",
        "inference(model, \"박서준이 다했따\")\n",
        "inference(model, \"꿀잠 잤습니다\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NVAg9E7F3c",
        "colab_type": "text"
      },
      "source": [
        "# # 3. 나만의 모델 만들어보기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMo6UCv78uGz",
        "colab_type": "text"
      },
      "source": [
        "# 실습 MISSION #16\n",
        ">  LSTM, Dense layer 등을 자유롭게 활용해서 자신만의 모델을 만들고 \n",
        "이후 TEST 데이터에 대해 최종 성능을 비교해보세요\n",
        "</font>\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EJxGxXl4MJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        " \n",
        "\n",
        "# 1. 모델 구현하기\n",
        "model2 = tf.keras.Sequential([\n",
        "\n",
        "      # MISSION 작성 #\n",
        "\n",
        "      ################                 \n",
        "\n",
        "]) \n",
        "\n",
        "# 2. optimizer, loss 선택하기\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. 모델 훈련하기\n",
        "num_epochs = 5\n",
        "history = model2.fit(train_input_ids, train_label_ids, epochs=num_epochs, validation_data=(val_input_ids, val_label_ids), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhibjqAt69vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. 모델 진단하기\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmDzbNJ7DfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. 테스트 데이터에 대해 평가하기\n",
        "\n",
        "model2.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2HnsxdPgRUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 샘플 예제에 대해 추론해 보세요 \n",
        "\n",
        "inference(model2, \"물이 반도 안남았다\")  #부정\n",
        "inference(model2, \"물이 반이나 남았다\")  #긍정\n",
        "inference(model2, \"죄송하지만 혹시 실례가 안된다면 꺼져주실수 있으신지ㅎㅎ?\") #부정\n",
        "inference(model2, \"잘하는 짓이다\") #부정\n",
        "inference(model2, \"가게 외관은 구린데 맛은 ㅇㅈ\") #긍정\n",
        "inference(model2, \"ㄷㄷ 간만에 갓띵작 ㄷㄷㄷ\") #긍정\n",
        "inference(model2, \"주인공 커여워 ㅠㅠ\") #긍정\n",
        "inference(model2, \"OTL\") #부정\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_E831p2KJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}