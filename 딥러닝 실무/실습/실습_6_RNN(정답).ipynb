{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_6_RNN(정답).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "한국어 자연어처리의 전반적인 FLOW를 이해하고, 간단한 감성분석 모델을 구현해 봅시다\n",
        "\n",
        "필요한 라이브러리를 import해줍니다.\n",
        "\n",
        "LSTM을 활용해 RNN 신경망을 구성하고, Embedding을 통해 단어를 벡터로 숫자로 바꿔주겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS"
      },
      "source": [
        "📋 교재로 돌아가 한국어 자연어처리 플로우를 보고 이해한 뒤 실습을 시작합시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfgQ96FEjKz"
      },
      "source": [
        "### Step 1. Tokenizing(Parsing)\n",
        "- 문장을 음절(character)단위로 쪼개기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXHdDffgaggl"
      },
      "source": [
        "def tokenize(sentence): \n",
        "  return [char for char in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv"
      },
      "source": [
        "sentence1 = \"시간 가는 줄 알고 봤습니다.\"\n",
        "sentence2 = \"안보면 후회ㅠㅠ...\"\n",
        "parsed_sent1 = tokenize(sentence1)\n",
        "parsed_sent2 = tokenize(sentence2)\n",
        "print(\"문장 1:\", parsed_sent1)\n",
        "print(\"문장 2:\", parsed_sent2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdzQGbXMFJNu"
      },
      "source": [
        "### Step 2. 모델 인풋 만들기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSdN7ROFdwN"
      },
      "source": [
        "#### 2-1) 음절 사전 만들기\n",
        "각 음절 캐릭터를 모델이 처리할 수 있는 정수 인덱스로 변환해야 합니다.\n",
        "- 캐릭터를 정수로 매핑하는 사전을 만들고,\n",
        "- 배치 연산을 위해 필요한 Padding([PAD])과 Out of vocabulary([OOV]) 토큰을 항상 맨 앞에 추가해줍니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a"
      },
      "source": [
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for word in parsed_sent1:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "for word in parsed_sent2:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "print(\"Vocab Dictionary Example:\")\n",
        "print(vocab_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCyqBSGYFjXx"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n",
        "- 위에서 만든 vocab_dict를 이용해 잘라놓은 문장을 모델에 태울 수 있는 정수 인덱스로 바꾸어줍니다\n",
        "- 기본적으로 LSTM은 가변적인 문장 길이를 인풋으로 받을 수 있지만, 배치 처리를 위해 <font color=\"blue\">max_seq_len</font>을 정해두고 길이를 통일합니다.\n",
        "    - max_seq_len 보다 짧은 문장에는 max_seq_len이 될 때까지 [PAD]에 해당하는 인덱스를 붙여줍니다\n",
        "    - max_seq_len 보다 긴 문장은 max_seq_len 개의 토큰만 남기고 자릅니다\n",
        "    - tensorflow.keras.preprocessing.sequence의 <font color=\"blue\">pad_sequences</font> 기능을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IfdgEZOEh3q"
      },
      "source": [
        "max_seq_len = 20\n",
        "\n",
        "input_id1 = [vocab_dict[word] for word in parsed_sent1]\n",
        "input_id2 = [vocab_dict[word] for word in parsed_sent2]\n",
        "\n",
        "# Padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "input_ids = [input_id1, input_id2]\n",
        "input_ids = pad_sequences(input_ids, maxlen=max_seq_len, value = vocab_dict['[PAD]']) \n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyE471IGDs1"
      },
      "source": [
        "### Step3. 모델 만들기\n",
        "\n",
        "- 임베딩 레이어 : Embedding()\n",
        "- LSTM : LSTM()\n",
        "- FC layer : Dense()   \n",
        "- LSTM을 사용해 문장을 인코딩하고, Dense layer을 두 층 쌓아 최종 output을 생성합시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMgZkCRJSaF2"
      },
      "source": [
        "vocab_size = len(vocab_dict)        # 단어사전 개수\n",
        "embedding_dim = 30     # 임베딩 차원\n",
        "lstm_hidden_dim = 50   # LSTM hidden_size \n",
        "dense_dim = 32         # FC layer size\n",
        "batch_size = 2         # batch size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(lstm_hidden_dim),\n",
        "    Dense(dense_dim, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btopfDwJv2l3"
      },
      "source": [
        "# LSTM으로 영화리뷰 감성분석 모델 훈련하기\n",
        "\n",
        "이제 본격적으로 영화리뷰를 다운받고 실습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJfJrzDPG1H"
      },
      "source": [
        "### Step 0. 학습 데이터 준비하기\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_5.PNG?raw=true\">    \n",
        "\n",
        "- 네이버 영화 감성분석 데이터셋 활용\n",
        "- 훈련 데이터 150,000건, 테스트 데이터 50,000건"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHygGkDeiyUZ"
      },
      "source": [
        "\"\"\" 네이버 영화 리뷰 데이터셋 다운로드 \"\"\"\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
        "\n",
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "    raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "    raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "    FULL_TRAIN.append([line[0], line[1], int(line[2].strip())])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "    FULL_TEST.append([line[0], line[1], int(line[2].strip())]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyE6Ia5uopA"
      },
      "source": [
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_6.PNG?raw=true\">  \n",
        "- 시간 관계상 train 중 50,000건을 학습데이터, 10,000건을 검증 데이터로 사용합니다.\n",
        "- test 중 10,000건만 샘플링하여 최종 성능 테스트에 사용하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_xG06TPI9vl"
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "print(\"train     : {}개 (긍정 {}, 부정 {})\".format(len(train), sum([t[2] for t in train]), len(train)-sum([t[2] for t in train])), train[0])\n",
        "print(\"validation: {}개 (긍정 {}, 부정 {})\".format(len(val), sum([t[2] for t in val]), len(val)-sum([t[2] for t in val])), val[0])\n",
        "print(\"test      : {}개 (긍정 {}, 부정 {})\".format(len(test), sum([t[2] for t in test]), len(test)-sum([t[2] for t in test])), test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IWVSphmZ42"
      },
      "source": [
        "라벨을 보니 0이 부정 리뷰, 1이 긍정 리뷰입니다. \n",
        "\n",
        "한국말은 끝까지 읽어봐야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jykZkziwnd2"
      },
      "source": [
        "## Step 1. Parsing\n",
        "- Train/ Test의 문장을 음절단위로 파싱합니다\n",
        "- 정답 라벨은 One-hot encoding 형식으로 저장합니다.\n",
        "   - 부정(0) -> [1, 0]\n",
        "   - 긍정(1) -> [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9HFloKROOnT"
      },
      "source": [
        "train_sentences = []\n",
        "val_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "# 추후 학습/ 테스트를 위해 라벨 정보 저장해둠\n",
        "train_label_ids = []\n",
        "val_label_ids = []\n",
        "test_label_ids = []\n",
        "\n",
        "print(\"start tokenizing TRAIN sentences\")\n",
        "for i, line in enumerate(train):\n",
        "    words = tokenize(line[1])\n",
        "    train_sentences.append(words)\n",
        "    if line[2] == 0: # 부정\n",
        "      train_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      train_label_ids.append([0,1])\n",
        "\n",
        "    if (i+1) % 5000 == 0: print(\"... {}/{} done\".format(i+1, len(train))) \n",
        "\n",
        "print(\"start tokenizing VALIDATION sentences\")\n",
        "\n",
        "for line in val:\n",
        "    words = tokenize(line[1])\n",
        "    val_sentences.append(words)\n",
        "    if line[2] == 0: # 부정\n",
        "      val_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      val_label_ids.append([0,1])\n",
        "print(\"... done\\n\")\n",
        "\n",
        "print(\"start tokenizing TEST sentences\")\n",
        "for line in test:\n",
        "    words = tokenize(line[1])\n",
        "    test_sentences.append(words)\n",
        "    if line[2] == 0: # 부정\n",
        "      test_label_ids.append([1,0])\n",
        "    else: #긍정\n",
        "      test_label_ids.append([0,1])\n",
        "\n",
        "print(\"... done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bDXwmsH2x6"
      },
      "source": [
        "##Step 2. 모델 인풋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFXWtJDWdH1E"
      },
      "source": [
        "#### 2-1) 음절 사전 만들기\n",
        "- 훈련 데이터 문장에 있는 음절을 이용해 사전을 만듭니다.\n",
        "- (일반적으로는 더 많은 코퍼스에 대해 구축된 사전을 사용하지만, 편의상 훈련셋만으로 진행합니다)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAfNMCpgwdww"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_dict.keys(): \n",
        "            vocab_dict[word] = i\n",
        "            i += 1\n",
        "print(\"Vocab Dictionary Size:\", len(vocab_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu_DabQjQYFK"
      },
      "source": [
        "#### 2-2) vocab_dict를 이용해 자연어를 정수 인덱스로 바꾸기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa7m_hYqM5x5"
      },
      "source": [
        "\n",
        "음절 단위로 쪼개진 문장들 (tokenized_sentences)을 인풋으로 받아 다음을 처리합니다.\n",
        "\n",
        "* 사전에 없는 음절은 [OOV] 인덱스로 처리하기   \n",
        "* 사전에서 매핑되는 음절은 해당 인덱스로 바꾸기\n",
        "* max_seq_len만큼 문장 길이 맞추고 이보다 짧은 문장은 [PAD] 인덱스로 패딩하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHI2uFCUojkw"
      },
      "source": [
        "def make_input_ids(tokenized_sentences, max_seq_len = 50):\n",
        "  \n",
        "  num_oov = 0 # OOV 발생 개수를 셈\n",
        "  result_input_ids = [] # result_input_ids : 정수 인덱스로 변환한 문장들의 리스트\n",
        "\n",
        "  for sentence in tokenized_sentences :\n",
        "      \"\"\" vocab_dict를 사용해 정수로 변환 \"\"\" \n",
        "      input_ids = []\n",
        "      for word in sentence:\n",
        "          if word not in vocab_dict:   ## 사전에 없는 음절은 OOV 처리\n",
        "              input_ids.append(vocab_dict['[OOV]']) \n",
        "              num_oov += 1\n",
        "          else:                       ## 사전에 있는 음절은?\n",
        "              input_ids.append(vocab_dict[word]) ##  vocab_dict 사전에서 토큰 찾아서 붙이기\n",
        "      \n",
        "      result_input_ids.append(input_ids)\n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  result_input_ids = pad_sequences(result_input_ids, maxlen=max_seq_len, value=vocab_dict[\"[PAD]\"]) ##  padding 하기\n",
        "\n",
        "  return result_input_ids, num_oov\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irm8oGg3ICWV"
      },
      "source": [
        "# train_sentences 처리\n",
        "train_input_ids, num_oov = make_input_ids(train_sentences)\n",
        "\n",
        "print(\"---- TRAIN ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFz1GGuxhGY"
      },
      "source": [
        "# val_sentences 처리\n",
        "val_input_ids, num_oov = make_input_ids(val_sentences)\n",
        "\n",
        "print(\"---- VALIDATION ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYs0YXRjIDg3"
      },
      "source": [
        "# test_sentences 처리\n",
        "test_input_ids, num_oov = make_input_ids(test_sentences)\n",
        "\n",
        "print(\"---- TEST ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7l98rF7c-Er"
      },
      "source": [
        "#### 2-3) 라벨 리스트를 np.array로 변환해줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuFQFHbj3AQd"
      },
      "source": [
        "train_label_ids = np.array(train_label_ids)\n",
        "val_label_ids = np.array(val_label_ids)\n",
        "test_label_ids = np.array(test_label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhcHsul1IHxj"
      },
      "source": [
        "## Step3. 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iPCdwGTjzk"
      },
      "source": [
        "## 실습 MISSION\n",
        "> 아래 조건에 맞는 모델을 만들어보고 학습하세요\n",
        " \n",
        "* embedding 차원은 150  \n",
        "* LSTM hidden size는 100\n",
        "* Dense의 hidden size는 100, relu activation 사용\n",
        "* output Dense layer에서는 긍/부정 2개 카테고리를 분류하되 softmax 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcDr8GlZTqqV"
      },
      "source": [
        "vocab_size = len(vocab_dict) \n",
        "\n",
        "model = Sequential([\n",
        "            ####### MISSION 작성 ######\n",
        "            Embedding(vocab_size, 150),\n",
        "            LSTM(100),\n",
        "            Dense(100, activation='relu'),\n",
        "            Dense(2, activation='softmax')\n",
        "            ###########################\n",
        "])  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GXrD_dTdTa_"
      },
      "source": [
        "## Step 4. 모델 훈련하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPp_wZ2ddaMG"
      },
      "source": [
        "#### 4-1) loss, optimizer를 지정하고 학습합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nPJiGW1q92"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCHS = 256\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_input_ids, train_label_ids, epochs=EPOCHS, batch_size=BATCHS, validation_data=(val_input_ids, val_label_ids), verbose=2) \n",
        "\n",
        "test_result = model.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MnPFs9O3Fr"
      },
      "source": [
        "#### 4-2) Learning Curve 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtaGxpJa4SmM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13AeKZe8JdIn"
      },
      "source": [
        "## Step 5. Inference 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxopYLQtIkw1"
      },
      "source": [
        "\"\"\" 학습된 모델로  예측해보기 \"\"\"\n",
        "\n",
        "def inference(mymodel, sentence):\n",
        "  # 1. tokenizer로 문장 파싱\n",
        "  words = tokenize(sentence)\n",
        "  input_id = []\n",
        "\n",
        "  # 2. vocab_dict를 이용해 인덱스로 변환\n",
        "  for word in words:\n",
        "    if word in vocab_dict: input_id.append(vocab_dict[word])\n",
        "    else: input_id.append(vocab_dict[\"[OOV]\"])\n",
        "  \n",
        "  # 단일 문장 추론이기 때문에 패딩할 필요가 없음 \n",
        "  score = mymodel.predict(np.array([input_id])) \n",
        "\n",
        "  print(\"** INPUT:\", sentence)\n",
        "  print(\"   -> 부정: {:.2f} / 긍정: {:.2f}\".format(score[0][0],score[0][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm172c7-YmBw"
      },
      "source": [
        "sentence1 = \"시간 가는 줄 알고 봤습니다.\"\n",
        "sentence2 = \"안보면 후회ㅠㅠ...\"\n",
        "inference(model, sentence1)\n",
        "inference(model, sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_fIiVC4k8g"
      },
      "source": [
        "# 원하는 문장에 대해 추론해 보세요\n",
        "inference(model, \"이런 망작을 나 혼자만 보기엔 아깝지\")\n",
        "inference(model, \"이런 꿀잼을 나 혼자만 보기엔 아깝지\")\n",
        "inference(model, \"꿀잠 잤습니다\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NVAg9E7F3c"
      },
      "source": [
        "# # 3. 나만의 모델 만들어보기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMo6UCv78uGz"
      },
      "source": [
        "# 실습 MISSION\n",
        ">  LSTM, Dense layer 등을 자유롭게 활용해서 자신만의 모델을 만들고 \n",
        "이후 TEST 데이터에 대해 최종 성능을 비교해보세요\n",
        "</font>\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EJxGxXl4MJF",
        "cellView": "both"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        " \n",
        "\n",
        "# 1. 모델 구현하기\n",
        "model2 = tf.keras.Sequential([\n",
        "      # MISSION 작성 #\n",
        "\n",
        "\n",
        "      ################                 \n",
        "]) \n",
        "\n",
        "# 2. optimizer, loss 선택하기\n",
        "model2.compile(loss='_______________', optimizer='________', metrics=['accuracy'])\n",
        "\n",
        "# 3. 모델 훈련하기 (원하는대로 조정해보세요!)\n",
        "num_epochs = 5\n",
        "num_batch = 256\n",
        "\n",
        "history = model2.fit(train_input_ids, train_label_ids, epochs=num_epochs, batch_size=num_batch, validation_data=(val_input_ids, val_label_ids), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhibjqAt69vc"
      },
      "source": [
        "# 4. 모델 진단하기\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmDzbNJ7DfC"
      },
      "source": [
        "# 5. 테스트 데이터에 대해 평가하기\n",
        "\n",
        "model2.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2HnsxdPgRUf"
      },
      "source": [
        "# 샘플 예제에 대해 추론해 보세요 \n",
        "\n",
        "inference(model2, \"물이 반도 안남았다\")  #부정\n",
        "inference(model2, \"물이 반이나 남았다\")  #긍정\n",
        "inference(model2, \"죄송하지만 혹시 실례가 안된다면 꺼져주실수 있으신지ㅎㅎ?\") #부정\n",
        "inference(model2, \"잘하는 짓이다\") #부정\n",
        "inference(model2, \"가게 외관은 구린데 맛은 ㅇㅈ\") #긍정\n",
        "inference(model2, \"ㄷㄷ 간만에 갓띵작 ㄷㄷㄷ\") #긍정\n",
        "inference(model2, \"주인공 커여워 ㅠㅠ\") #긍정\n",
        "inference(model2, \"OTL\") #부정\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_E831p2KJ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}