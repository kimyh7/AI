{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ì‹¤ìŠµ_6_RNN(ì •ë‹µ).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ì˜ ì „ë°˜ì ì¸ FLOWë¥¼ ì´í•´í•˜ê³ , ê°„ë‹¨í•œ ê°ì„±ë¶„ì„ ëª¨ë¸ì„ êµ¬í˜„í•´ ë´…ì‹œë‹¤\n",
        "\n",
        "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•´ì¤ë‹ˆë‹¤.\n",
        "\n",
        "LSTMì„ í™œìš©í•´ RNN ì‹ ê²½ë§ì„ êµ¬ì„±í•˜ê³ , Embeddingì„ í†µí•´ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ìˆ«ìë¡œ ë°”ê¿”ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS"
      },
      "source": [
        "ğŸ“‹ êµì¬ë¡œ ëŒì•„ê°€ í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ í”Œë¡œìš°ë¥¼ ë³´ê³  ì´í•´í•œ ë’¤ ì‹¤ìŠµì„ ì‹œì‘í•©ì‹œë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RfgQ96FEjKz"
      },
      "source": [
        "### Step 1. Tokenizing(Parsing)\n",
        "- ë¬¸ì¥ì„ ìŒì ˆ(character)ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXHdDffgaggl"
      },
      "source": [
        "def tokenize(sentence): \n",
        "  return [char for char in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv"
      },
      "source": [
        "sentence1 = \"ì‹œê°„ ê°€ëŠ” ì¤„ ì•Œê³  ë´¤ìŠµë‹ˆë‹¤.\"\n",
        "sentence2 = \"ì•ˆë³´ë©´ í›„íšŒã… ã… ...\"\n",
        "parsed_sent1 = tokenize(sentence1)\n",
        "parsed_sent2 = tokenize(sentence2)\n",
        "print(\"ë¬¸ì¥ 1:\", parsed_sent1)\n",
        "print(\"ë¬¸ì¥ 2:\", parsed_sent2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdzQGbXMFJNu"
      },
      "source": [
        "### Step 2. ëª¨ë¸ ì¸í’‹ ë§Œë“¤ê¸°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSdN7ROFdwN"
      },
      "source": [
        "#### 2-1) ìŒì ˆ ì‚¬ì „ ë§Œë“¤ê¸°\n",
        "ê° ìŒì ˆ ìºë¦­í„°ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ìºë¦­í„°ë¥¼ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ë§Œë“¤ê³ ,\n",
        "- ë°°ì¹˜ ì—°ì‚°ì„ ìœ„í•´ í•„ìš”í•œ Padding([PAD])ê³¼ Out of vocabulary([OOV]) í† í°ì„ í•­ìƒ ë§¨ ì•ì— ì¶”ê°€í•´ì¤ë‹ˆë‹¤"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a"
      },
      "source": [
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for word in parsed_sent1:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "for word in parsed_sent2:\n",
        "    if word not in vocab_dict.keys():\n",
        "        vocab_dict[word] = i\n",
        "        i += 1\n",
        "print(\"Vocab Dictionary Example:\")\n",
        "print(vocab_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCyqBSGYFjXx"
      },
      "source": [
        "#### 2-2) vocab_dictë¥¼ ì´ìš©í•´ ìì—°ì–´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "- ìœ„ì—ì„œ ë§Œë“  vocab_dictë¥¼ ì´ìš©í•´ ì˜ë¼ë†“ì€ ë¬¸ì¥ì„ ëª¨ë¸ì— íƒœìš¸ ìˆ˜ ìˆëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ì–´ì¤ë‹ˆë‹¤\n",
        "- ê¸°ë³¸ì ìœ¼ë¡œ LSTMì€ ê°€ë³€ì ì¸ ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¸í’‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ <font color=\"blue\">max_seq_len</font>ì„ ì •í•´ë‘ê³  ê¸¸ì´ë¥¼ í†µì¼í•©ë‹ˆë‹¤.\n",
        "    - max_seq_len ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì—ëŠ” max_seq_lenì´ ë  ë•Œê¹Œì§€ [PAD]ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤\n",
        "    - max_seq_len ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ max_seq_len ê°œì˜ í† í°ë§Œ ë‚¨ê¸°ê³  ìë¦…ë‹ˆë‹¤\n",
        "    - tensorflow.keras.preprocessing.sequenceì˜ <font color=\"blue\">pad_sequences</font> ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IfdgEZOEh3q"
      },
      "source": [
        "max_seq_len = 20\n",
        "\n",
        "input_id1 = [vocab_dict[word] for word in parsed_sent1]\n",
        "input_id2 = [vocab_dict[word] for word in parsed_sent2]\n",
        "\n",
        "# Padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "input_ids = [input_id1, input_id2]\n",
        "input_ids = pad_sequences(input_ids, maxlen=max_seq_len, value = vocab_dict['[PAD]']) \n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyE471IGDs1"
      },
      "source": [
        "### Step3. ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "\n",
        "- ì„ë² ë”© ë ˆì´ì–´ : Embedding()\n",
        "- LSTM : LSTM()\n",
        "- FC layer : Dense()   \n",
        "- LSTMì„ ì‚¬ìš©í•´ ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ê³ , Dense layerì„ ë‘ ì¸µ ìŒ“ì•„ ìµœì¢… outputì„ ìƒì„±í•©ì‹œë‹¤"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMgZkCRJSaF2"
      },
      "source": [
        "vocab_size = len(vocab_dict)        # ë‹¨ì–´ì‚¬ì „ ê°œìˆ˜\n",
        "embedding_dim = 30     # ì„ë² ë”© ì°¨ì›\n",
        "lstm_hidden_dim = 50   # LSTM hidden_size \n",
        "dense_dim = 32         # FC layer size\n",
        "batch_size = 2         # batch size\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(lstm_hidden_dim),\n",
        "    Dense(dense_dim, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btopfDwJv2l3"
      },
      "source": [
        "# LSTMìœ¼ë¡œ ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ ëª¨ë¸ í›ˆë ¨í•˜ê¸°\n",
        "\n",
        "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì˜í™”ë¦¬ë·°ë¥¼ ë‹¤ìš´ë°›ê³  ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJfJrzDPG1H"
      },
      "source": [
        "### Step 0. í•™ìŠµ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_5.PNG?raw=true\">    \n",
        "\n",
        "- ë„¤ì´ë²„ ì˜í™” ê°ì„±ë¶„ì„ ë°ì´í„°ì…‹ í™œìš©\n",
        "- í›ˆë ¨ ë°ì´í„° 150,000ê±´, í…ŒìŠ¤íŠ¸ ë°ì´í„° 50,000ê±´"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHygGkDeiyUZ"
      },
      "source": [
        "\"\"\" ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ \"\"\"\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
        "\n",
        "\"\"\" ë°ì´í„° ì½ì–´ì˜¤ê¸° \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "    raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "    raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "    FULL_TRAIN.append([line[0], line[1], int(line[2].strip())])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "    FULL_TEST.append([line[0], line[1], int(line[2].strip())]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyE6Ia5uopA"
      },
      "source": [
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/image_6.PNG?raw=true\">  \n",
        "- ì‹œê°„ ê´€ê³„ìƒ train ì¤‘ 50,000ê±´ì„ í•™ìŠµë°ì´í„°, 10,000ê±´ì„ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "- test ì¤‘ 10,000ê±´ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_xG06TPI9vl"
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "print(\"train     : {}ê°œ (ê¸ì • {}, ë¶€ì • {})\".format(len(train), sum([t[2] for t in train]), len(train)-sum([t[2] for t in train])), train[0])\n",
        "print(\"validation: {}ê°œ (ê¸ì • {}, ë¶€ì • {})\".format(len(val), sum([t[2] for t in val]), len(val)-sum([t[2] for t in val])), val[0])\n",
        "print(\"test      : {}ê°œ (ê¸ì • {}, ë¶€ì • {})\".format(len(test), sum([t[2] for t in test]), len(test)-sum([t[2] for t in test])), test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IWVSphmZ42"
      },
      "source": [
        "ë¼ë²¨ì„ ë³´ë‹ˆ 0ì´ ë¶€ì • ë¦¬ë·°, 1ì´ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤. \n",
        "\n",
        "í•œêµ­ë§ì€ ëê¹Œì§€ ì½ì–´ë´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jykZkziwnd2"
      },
      "source": [
        "## Step 1. Parsing\n",
        "- Train/ Testì˜ ë¬¸ì¥ì„ ìŒì ˆë‹¨ìœ„ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤\n",
        "- ì •ë‹µ ë¼ë²¨ì€ One-hot encoding í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "   - ë¶€ì •(0) -> [1, 0]\n",
        "   - ê¸ì •(1) -> [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9HFloKROOnT"
      },
      "source": [
        "train_sentences = []\n",
        "val_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "# ì¶”í›„ í•™ìŠµ/ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¼ë²¨ ì •ë³´ ì €ì¥í•´ë‘ \n",
        "train_label_ids = []\n",
        "val_label_ids = []\n",
        "test_label_ids = []\n",
        "\n",
        "print(\"start tokenizing TRAIN sentences\")\n",
        "for i, line in enumerate(train):\n",
        "    words = tokenize(line[1])\n",
        "    train_sentences.append(words)\n",
        "    if line[2] == 0: # ë¶€ì •\n",
        "      train_label_ids.append([1,0])\n",
        "    else: #ê¸ì •\n",
        "      train_label_ids.append([0,1])\n",
        "\n",
        "    if (i+1) % 5000 == 0: print(\"... {}/{} done\".format(i+1, len(train))) \n",
        "\n",
        "print(\"start tokenizing VALIDATION sentences\")\n",
        "\n",
        "for line in val:\n",
        "    words = tokenize(line[1])\n",
        "    val_sentences.append(words)\n",
        "    if line[2] == 0: # ë¶€ì •\n",
        "      val_label_ids.append([1,0])\n",
        "    else: #ê¸ì •\n",
        "      val_label_ids.append([0,1])\n",
        "print(\"... done\\n\")\n",
        "\n",
        "print(\"start tokenizing TEST sentences\")\n",
        "for line in test:\n",
        "    words = tokenize(line[1])\n",
        "    test_sentences.append(words)\n",
        "    if line[2] == 0: # ë¶€ì •\n",
        "      test_label_ids.append([1,0])\n",
        "    else: #ê¸ì •\n",
        "      test_label_ids.append([0,1])\n",
        "\n",
        "print(\"... done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bDXwmsH2x6"
      },
      "source": [
        "##Step 2. ëª¨ë¸ ì¸í’‹ ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFXWtJDWdH1E"
      },
      "source": [
        "#### 2-1) ìŒì ˆ ì‚¬ì „ ë§Œë“¤ê¸°\n",
        "- í›ˆë ¨ ë°ì´í„° ë¬¸ì¥ì— ìˆëŠ” ìŒì ˆì„ ì´ìš©í•´ ì‚¬ì „ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "- (ì¼ë°˜ì ìœ¼ë¡œëŠ” ë” ë§ì€ ì½”í¼ìŠ¤ì— ëŒ€í•´ êµ¬ì¶•ëœ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì§€ë§Œ, í¸ì˜ìƒ í›ˆë ¨ì…‹ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAfNMCpgwdww"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "vocab_dict = {}\n",
        "vocab_dict[\"[PAD]\"] = 0\n",
        "vocab_dict[\"[OOV]\"] = 1\n",
        "i = 2\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocab_dict.keys(): \n",
        "            vocab_dict[word] = i\n",
        "            i += 1\n",
        "print(\"Vocab Dictionary Size:\", len(vocab_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu_DabQjQYFK"
      },
      "source": [
        "#### 2-2) vocab_dictë¥¼ ì´ìš©í•´ ìì—°ì–´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa7m_hYqM5x5"
      },
      "source": [
        "\n",
        "ìŒì ˆ ë‹¨ìœ„ë¡œ ìª¼ê°œì§„ ë¬¸ì¥ë“¤ (tokenized_sentences)ì„ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ ë‹¤ìŒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "* ì‚¬ì „ì— ì—†ëŠ” ìŒì ˆì€ [OOV] ì¸ë±ìŠ¤ë¡œ ì²˜ë¦¬í•˜ê¸°   \n",
        "* ì‚¬ì „ì—ì„œ ë§¤í•‘ë˜ëŠ” ìŒì ˆì€ í•´ë‹¹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "* max_seq_lenë§Œí¼ ë¬¸ì¥ ê¸¸ì´ ë§ì¶”ê³  ì´ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì€ [PAD] ì¸ë±ìŠ¤ë¡œ íŒ¨ë”©í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHI2uFCUojkw"
      },
      "source": [
        "def make_input_ids(tokenized_sentences, max_seq_len = 50):\n",
        "  \n",
        "  num_oov = 0 # OOV ë°œìƒ ê°œìˆ˜ë¥¼ ì…ˆ\n",
        "  result_input_ids = [] # result_input_ids : ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•œ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "  for sentence in tokenized_sentences :\n",
        "      \"\"\" vocab_dictë¥¼ ì‚¬ìš©í•´ ì •ìˆ˜ë¡œ ë³€í™˜ \"\"\" \n",
        "      input_ids = []\n",
        "      for word in sentence:\n",
        "          if word not in vocab_dict:   ## ì‚¬ì „ì— ì—†ëŠ” ìŒì ˆì€ OOV ì²˜ë¦¬\n",
        "              input_ids.append(vocab_dict['[OOV]']) \n",
        "              num_oov += 1\n",
        "          else:                       ## ì‚¬ì „ì— ìˆëŠ” ìŒì ˆì€?\n",
        "              input_ids.append(vocab_dict[word]) ##  vocab_dict ì‚¬ì „ì—ì„œ í† í° ì°¾ì•„ì„œ ë¶™ì´ê¸°\n",
        "      \n",
        "      result_input_ids.append(input_ids)\n",
        "      \n",
        "  \"\"\" max_seq_lenì„ ë„˜ëŠ” ë¬¸ì¥ì€ ì ˆë‹¨, ëª¨ìë¥´ëŠ” ê²ƒì€ PADDING \"\"\"\n",
        "  result_input_ids = pad_sequences(result_input_ids, maxlen=max_seq_len, value=vocab_dict[\"[PAD]\"]) ##  padding í•˜ê¸°\n",
        "\n",
        "  return result_input_ids, num_oov\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irm8oGg3ICWV"
      },
      "source": [
        "# train_sentences ì²˜ë¦¬\n",
        "train_input_ids, num_oov = make_input_ids(train_sentences)\n",
        "\n",
        "print(\"---- TRAIN ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFz1GGuxhGY"
      },
      "source": [
        "# val_sentences ì²˜ë¦¬\n",
        "val_input_ids, num_oov = make_input_ids(val_sentences)\n",
        "\n",
        "print(\"---- VALIDATION ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYs0YXRjIDg3"
      },
      "source": [
        "# test_sentences ì²˜ë¦¬\n",
        "test_input_ids, num_oov = make_input_ids(test_sentences)\n",
        "\n",
        "print(\"---- TEST ----\")\n",
        "print(\"... # OOVs     :\", num_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7l98rF7c-Er"
      },
      "source": [
        "#### 2-3) ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¥¼ np.arrayë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuFQFHbj3AQd"
      },
      "source": [
        "train_label_ids = np.array(train_label_ids)\n",
        "val_label_ids = np.array(val_label_ids)\n",
        "test_label_ids = np.array(test_label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhcHsul1IHxj"
      },
      "source": [
        "## Step3. ëª¨ë¸ ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iPCdwGTjzk"
      },
      "source": [
        "## ì‹¤ìŠµ MISSION\n",
        "> ì•„ë˜ ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê³  í•™ìŠµí•˜ì„¸ìš”\n",
        " \n",
        "* embedding ì°¨ì›ì€ 150  \n",
        "* LSTM hidden sizeëŠ” 100\n",
        "* Denseì˜ hidden sizeëŠ” 100, relu activation ì‚¬ìš©\n",
        "* output Dense layerì—ì„œëŠ” ê¸/ë¶€ì • 2ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ë˜ softmax ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcDr8GlZTqqV"
      },
      "source": [
        "vocab_size = len(vocab_dict) \n",
        "\n",
        "model = Sequential([\n",
        "            ####### MISSION ì‘ì„± ######\n",
        "            Embedding(vocab_size, 150),\n",
        "            LSTM(100),\n",
        "            Dense(100, activation='relu'),\n",
        "            Dense(2, activation='softmax')\n",
        "            ###########################\n",
        "])  \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GXrD_dTdTa_"
      },
      "source": [
        "## Step 4. ëª¨ë¸ í›ˆë ¨í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPp_wZ2ddaMG"
      },
      "source": [
        "#### 4-1) loss, optimizerë¥¼ ì§€ì •í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3nPJiGW1q92"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCHS = 256\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_input_ids, train_label_ids, epochs=EPOCHS, batch_size=BATCHS, validation_data=(val_input_ids, val_label_ids), verbose=2) \n",
        "\n",
        "test_result = model.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MnPFs9O3Fr"
      },
      "source": [
        "#### 4-2) Learning Curve í™•ì¸í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtaGxpJa4SmM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13AeKZe8JdIn"
      },
      "source": [
        "## Step 5. Inference ì‹¤í–‰í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxopYLQtIkw1"
      },
      "source": [
        "\"\"\" í•™ìŠµëœ ëª¨ë¸ë¡œ  ì˜ˆì¸¡í•´ë³´ê¸° \"\"\"\n",
        "\n",
        "def inference(mymodel, sentence):\n",
        "  # 1. tokenizerë¡œ ë¬¸ì¥ íŒŒì‹±\n",
        "  words = tokenize(sentence)\n",
        "  input_id = []\n",
        "\n",
        "  # 2. vocab_dictë¥¼ ì´ìš©í•´ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
        "  for word in words:\n",
        "    if word in vocab_dict: input_id.append(vocab_dict[word])\n",
        "    else: input_id.append(vocab_dict[\"[OOV]\"])\n",
        "  \n",
        "  # ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡ ì´ê¸° ë•Œë¬¸ì— íŒ¨ë”©í•  í•„ìš”ê°€ ì—†ìŒ \n",
        "  score = mymodel.predict(np.array([input_id])) \n",
        "\n",
        "  print(\"** INPUT:\", sentence)\n",
        "  print(\"   -> ë¶€ì •: {:.2f} / ê¸ì •: {:.2f}\".format(score[0][0],score[0][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm172c7-YmBw"
      },
      "source": [
        "sentence1 = \"ì‹œê°„ ê°€ëŠ” ì¤„ ì•Œê³  ë´¤ìŠµë‹ˆë‹¤.\"\n",
        "sentence2 = \"ì•ˆë³´ë©´ í›„íšŒã… ã… ...\"\n",
        "inference(model, sentence1)\n",
        "inference(model, sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_fIiVC4k8g"
      },
      "source": [
        "# ì›í•˜ëŠ” ë¬¸ì¥ì— ëŒ€í•´ ì¶”ë¡ í•´ ë³´ì„¸ìš”\n",
        "inference(model, \"ì´ëŸ° ë§ì‘ì„ ë‚˜ í˜¼ìë§Œ ë³´ê¸°ì—” ì•„ê¹ì§€\")\n",
        "inference(model, \"ì´ëŸ° ê¿€ì¼ì„ ë‚˜ í˜¼ìë§Œ ë³´ê¸°ì—” ì•„ê¹ì§€\")\n",
        "inference(model, \"ê¿€ì  ì¤ìŠµë‹ˆë‹¤\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NVAg9E7F3c"
      },
      "source": [
        "# # 3. ë‚˜ë§Œì˜ ëª¨ë¸ ë§Œë“¤ì–´ë³´ê¸° "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMo6UCv78uGz"
      },
      "source": [
        "# ì‹¤ìŠµ MISSION\n",
        ">  LSTM, Dense layer ë“±ì„ ììœ ë¡­ê²Œ í™œìš©í•´ì„œ ìì‹ ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ê³  \n",
        "ì´í›„ TEST ë°ì´í„°ì— ëŒ€í•´ ìµœì¢… ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”\n",
        "</font>\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EJxGxXl4MJF",
        "cellView": "both"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        " \n",
        "\n",
        "# 1. ëª¨ë¸ êµ¬í˜„í•˜ê¸°\n",
        "model2 = tf.keras.Sequential([\n",
        "      # MISSION ì‘ì„± #\n",
        "\n",
        "\n",
        "      ################                 \n",
        "]) \n",
        "\n",
        "# 2. optimizer, loss ì„ íƒí•˜ê¸°\n",
        "model2.compile(loss='_______________', optimizer='________', metrics=['accuracy'])\n",
        "\n",
        "# 3. ëª¨ë¸ í›ˆë ¨í•˜ê¸° (ì›í•˜ëŠ”ëŒ€ë¡œ ì¡°ì •í•´ë³´ì„¸ìš”!)\n",
        "num_epochs = 5\n",
        "num_batch = 256\n",
        "\n",
        "history = model2.fit(train_input_ids, train_label_ids, epochs=num_epochs, batch_size=num_batch, validation_data=(val_input_ids, val_label_ids), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhibjqAt69vc"
      },
      "source": [
        "# 4. ëª¨ë¸ ì§„ë‹¨í•˜ê¸°\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmDzbNJ7DfC"
      },
      "source": [
        "# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ í‰ê°€í•˜ê¸°\n",
        "\n",
        "model2.evaluate(test_input_ids, test_label_ids, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2HnsxdPgRUf"
      },
      "source": [
        "# ìƒ˜í”Œ ì˜ˆì œì— ëŒ€í•´ ì¶”ë¡ í•´ ë³´ì„¸ìš” \n",
        "\n",
        "inference(model2, \"ë¬¼ì´ ë°˜ë„ ì•ˆë‚¨ì•˜ë‹¤\")  #ë¶€ì •\n",
        "inference(model2, \"ë¬¼ì´ ë°˜ì´ë‚˜ ë‚¨ì•˜ë‹¤\")  #ê¸ì •\n",
        "inference(model2, \"ì£„ì†¡í•˜ì§€ë§Œ í˜¹ì‹œ ì‹¤ë¡€ê°€ ì•ˆëœë‹¤ë©´ êº¼ì ¸ì£¼ì‹¤ìˆ˜ ìˆìœ¼ì‹ ì§€ã…ã…?\") #ë¶€ì •\n",
        "inference(model2, \"ì˜í•˜ëŠ” ì§“ì´ë‹¤\") #ë¶€ì •\n",
        "inference(model2, \"ê°€ê²Œ ì™¸ê´€ì€ êµ¬ë¦°ë° ë§›ì€ ã…‡ã…ˆ\") #ê¸ì •\n",
        "inference(model2, \"ã„·ã„· ê°„ë§Œì— ê°“ëµì‘ ã„·ã„·ã„·\") #ê¸ì •\n",
        "inference(model2, \"ì£¼ì¸ê³µ ì»¤ì—¬ì›Œ ã… ã… \") #ê¸ì •\n",
        "inference(model2, \"OTL\") #ë¶€ì •\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_E831p2KJ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}