{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_1_단어사전_만들기_정답.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EhbQCFOtd5W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX8AhdQgtM6s"
      },
      "source": [
        "(🎧) 표시가 되어 있는 부분은 Wire 페이지에서 오디오 코드 해설을 제공하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEYUqFftYIM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "# 실습 1. 단어사전 만들기\n",
        "\n",
        "단어사전을 만드는 것은 NLU 모델링의 시작이자 가장 중요한 부분 중 하나입니다.   \n",
        "모델은 단어사전에 있는 토큰만을 인식할 수 있고, 이 사전을 토대로 Language와 태스크를 배우기 때문입니다.\n",
        "\n",
        "\n",
        "<b>학습 목표:    \n",
        "- 자연어처리의 기본인 토크나이징의 개념을 이해한다.\n",
        "- 딥러닝을 위한 단어사전을 구축할 때 유의할 점을 파악한다.</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaPo6v0x3Ss"
      },
      "source": [
        "## #0. 실습 준비하기\n",
        "구글 드라이브에 NLP 폴더를 모두 생성해두셨나요?   \n",
        "단어사전을 만든 후 드라이브의 NLP 폴더에 저장하기 위해 Colab을 구글 드라이브와 연동하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYwGgmLfJOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce23c0ef-8a81-4e7b-fedf-d2bb1f1e348d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxedaxc2zfVR"
      },
      "source": [
        "#### > 라이브러리 설치\n",
        "\n",
        "konlpy라는 한국어 자연어처리 라이브러리를 사용해 자연어를 형태소 단위로 분석할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSzlwmMXzgzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bccac29e-5d14-45b1-9ea5-fc2c60755779"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0phzT8czgfm"
      },
      "source": [
        "#### > 데이터 준비하기\n",
        "실습에서 사용할 데이터셋을 다운받아 읽어오는 부분입니다.   \n",
        "\n",
        "Source Data :\n",
        "- 한국어 위키백과 단락 일부\n",
        "- KorQuAD 1.0 데이터 본문 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eYDK6X4yBfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3048b0b-bb73-4c04-c982-a65791e88eeb"
      },
      "source": [
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"/content/KorQuAD_v1.0_train.json\") as f:\n",
        "  data = json.loads(f.read())\n",
        "\n",
        "PARAS = []\n",
        "for dat in data[\"data\"]:\n",
        "  for para in dat[\"paragraphs\"]:\n",
        "    PARAS.append(para[\"context\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-16 06:31:10--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38527475 (37M) [application/json]\n",
            "Saving to: ‘KorQuAD_v1.0_train.json’\n",
            "\n",
            "KorQuAD_v1.0_train. 100%[===================>]  36.74M  72.5MB/s    in 0.5s    \n",
            "\n",
            "2020-11-16 06:31:11 (72.5 MB/s) - ‘KorQuAD_v1.0_train.json’ saved [38527475/38527475]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQT9jE5elxGU"
      },
      "source": [
        "👉 다운로드한 데이터셋의 첫 번째 문단을 프린트해보겠습니다.    괴테의 파우스트와 관련된 위키백과 페이지네요.   \n",
        "\n",
        "👉 위키백과는 이러한 문학 뿐만 아니라 정치, 경제, 사회, 과학 전반을 아우르는 다양한 주제가 포함되어 있기 때문에   \n",
        "토큰 임베딩 사전학습에 사용할 수 있는 좋은 코퍼스 중 하나라고 볼 수 있습니다.   \n",
        "\n",
        "👉 총 9,681개의 문단이 데이터셋에 포함되어 있는 것도 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeyLY9cC8lj8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "2f2cbf6a-b0c4-4b01-96ed-b51f5d34fd6d"
      },
      "source": [
        "print(\"** Number of Paragraphs:\", len(PARAS))\n",
        "PARAS[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Number of Paragraphs: 9681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJb4WdIdz9qK"
      },
      "source": [
        "## #1. 토크나이징\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step1.PNG?raw=true\">\n",
        "\n",
        "- konply : 한국어 자연어처리 관련 패키지\n",
        "- konply의 Komoran tagger을 이용해 형태소 분석 실행\n",
        "   \n",
        "   \n",
        "이제 본격적으로 konlpy 패키지를 이용해 형태소분석을 실시해보겠습니다.   \n",
        "konlpy에는 Komoran, Hannanum, Kkma, Okt 등 다양한 분석기가 포함되어 있습니다.   \n",
        "형태소 분석기를 호출한 후 tokenizer.morphs() 함수를 통해 분석 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8uFroJgKwi"
      },
      "source": [
        "from konlpy.tag import Komoran, Hannanum, Kkma, Okt\n",
        "komoran = Komoran()\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "\n",
        "# 문장, 형태소분석기를 인풋으로 받아 쪼개진 문장을 리턴하는 함수 정의\n",
        "def tokenize(sentence, tokenizer):\n",
        "  return tokenizer.morphs(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MtMztHb2KcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f3f917-6764-4040-f54d-1c61fa7dfa56"
      },
      "source": [
        "sentence = \"형태소 분석기마다 쪼개진 문장 특성이 조금씩 달라요\"\n",
        "print(\"한나눔:\", tokenize(sentence, hannanum))\n",
        "print(\"꼬꼬마:\", tokenize(sentence, kkma))\n",
        "print(\"코모란:\", tokenize(sentence, komoran))\n",
        "print(\"트위터:\", tokenize(sentence, okt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "한나눔: ['형태소', '분석기', '마다', '쪼개', '어', '지', 'ㄴ', '문장', '특성', '이', '조금씩', '달라요']\n",
            "꼬꼬마: ['형태소', '분석기', '마다', '쪼개지', 'ㄴ', '문장', '특성', '이', '조금씩', '닿', 'ㄹ라요']\n",
            "코모란: ['형태소', '분석기', '마다', '쪼개', '어', '지', 'ㄴ', '문장', '특성', '이', '조금', '씩', '다르', '아요']\n",
            "트위터: ['형태소', '분석', '기마', '다', '쪼개진', '문장', '특성', '이', '조금씩', '달라', '요']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVZlVJSnDVQ"
      },
      "source": [
        "형태소분석기마다 분석 결과가 조금씩 다른 것을 볼 수 있습니다.   \n",
        "\n",
        "👉한나눔, 꼬꼬마, 코모란 분석기는 원형복원을 하기 때문에, 형태소의 원형이 복원된 결과를 리턴합니다.   \n",
        "👉트위터분석기는 들어온 문장을 형태소 단위로 쪼개서 그대로 리턴하고 있습니다.   \n",
        "\n",
        "우리는 실습에서 속도가 비교적 빠른 komoran 분석기를 사용할 것입니다.   \n",
        "코모란 분석기를 사용해서 문장을 인풋으로 받아 쪼개진 리스트를 리턴하는 tokenize 함수를 직접 작성해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyDRSSZU3L1_"
      },
      "source": [
        "<font color = \"red\">[MISSION] 코모란 분석기를 이용해 문장을 인풋으로 받아 형태소 분석된 리스트를 반환하는 tokenize 함수를 구현해보세요</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1012e00b-aa1b-4c76-bdc2-4191529b59c4"
      },
      "source": [
        "komoran = Komoran()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return komoran.morphs(sentence)\n",
        "\n",
        "tokenize(\"미션 완료!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미션', '완료', '!!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdzQGbXMFJNu"
      },
      "source": [
        "## #2. 단어 사전 만들기\n",
        "\n",
        "👏자연어가 들어올 때 형태소 분석을 완료하고, 분석된 결과를 리턴할 수 있는 함수를 정의했습니다.   \n",
        "\n",
        "이제 분석된 형태소를 모델이 처리할 수 있는 정수 인덱스로 변환해야 합니다.   \n",
        "그리고 이 과정에는 단어사전이 반드시 필요하지요.\n",
        "\n",
        "🙆‍♀️단어사전을 만들 때 고려해야 할 사항, 기억하시나요?\n",
        "\n",
        "1.  몇 개의 토큰을 사전에 포함할 것인가?\n",
        "2.  배치 연산을 위해 필요한 Padding([PAD])과 Out of vocabulary([UNK]) 토큰을 반드시 포함해야 함\n",
        "\n",
        "이 두 가지를 염두에 두고, 이제 단어사전을 만들어보겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2aA5Z4x4aPU"
      },
      "source": [
        "#### Step 1. 학습 코퍼스에 있는 형태소 토큰의 Frequency 체크하기 \n",
        "\n",
        "먼저 학습 코퍼스에 있는 토큰이 어떤 것들이 있는지, 몇 번씩 등장하는지 체크해야 합니다.   \n",
        "\n",
        "모든 토큰을 단어사전에 포함하면 자칫 임베딩 크기가 너무 커질 수 있기 때문에    \n",
        "빈도수가 낮은 토큰은 제외하고 싶을 수 있기 때문이지요.   \n",
        "\n",
        "\n",
        "우리는 총 9,681개 위키백과 문단들을 학습 코퍼스로 학습했는데요,   \n",
        "이 중에서 8,000개 문단은 학습 데이터로, 나머지는 검증용 데이터로 나눠서 모델링에 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sMQbNuk-qFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29aff2e8-c459-44c1-f75d-879a334d67a2"
      },
      "source": [
        "# 전체 문단을 Train과 Validation으로 나누기\n",
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(PARAS)\n",
        "\n",
        "PARAS_tr = PARAS[:8000]\n",
        "PARAS_dev = PARAS[8000:]\n",
        "\n",
        "print(\"Train: {} | Val: {}\".format(len(PARAS_tr), len(PARAS_dev)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 8000 | Val: 1681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IH09vVKbyom"
      },
      "source": [
        "## 분할한 데이터셋 구글 드라이브에 저장\n",
        "import json\n",
        "with open(\"/content/gdrive/My Drive/NLP/CBOW_train_paras.json\" , 'w') as f:\n",
        "  f.write(json.dumps(PARAS_tr))\n",
        "with open(\"/content/gdrive/My Drive/NLP/CBOW_dev_paras.json\" , 'w') as f:\n",
        "  f.write(json.dumps(PARAS_dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--I4ZNU1qI8r"
      },
      "source": [
        "다음으로 학습 코퍼스의 각각의 문단을 형태소 단위로 쪼개고, 등장한 형태소 토큰의 빈도수를 체크하겠습니다.   \n",
        "collections 라이브러리의 Counter 기능을 사용하면 간단하게 할 수 있습니다. (🎧) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw-VfzBe4YU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "6c0f4bdb-445a-48d0-af35-0623de519283"
      },
      "source": [
        "# 훈련 셋에 있는 Paragraph를 형태소로 쪼개고,토큰의 빈도를 체크하기 \n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "vocab_freq = Counter()\n",
        "TOKEN_PARAS_tr = []\n",
        "for i,para in tqdm(enumerate(PARAS_tr)):\n",
        "  tokenized_para = tokenize(para)\n",
        "  TOKEN_PARAS_tr.append(tokenized_para)\n",
        "  if i == 0: print(\"\\n... tokenized_para 예시:\", tokenized_para)\n",
        "  for word in tokenized_para:\n",
        "    vocab_freq[word] += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4it [00:00, 34.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "... tokenized_para 예시: ['차량', '정비', '소가', '있', '는', '패턴', '에', '만', '있', '다', '.', '화가', '로', '추정', '되', '는', '중립', '민간인', '노숙자', 'NPC', '가', '1', '명', '있', '고', '음식', '을', '구걸', '하', '는데', ',', '종류', '에', '상관', '없이', '음식', '을', '주', '면', '지하', '에', '숨기', '어', '지', 'ㄴ', '물품', '더미', '의', '존재', '를', '알리', '어', '주', '고', '사기', '가', '올라가', 'ㄴ다', '.', '노숙자', '가', '알리', '어', '주', 'ㄴ', '물품', '더미', '에서', '보석', '과', '알콜', '을', '각각', '1', '개', '씩', '얻', '을', '수', '있', '다', '.', '돕', '지', '않', '으면', '몇', '차례', '방문', '뒤', '결국', '노숙자', '는', '굶', '어', '죽', '어', '그', '시체', '를', '가져오', 'ㄹ', '수', '있', '다', '.', '노숙자', '가', '나타나', 'ㄴ', '뒤', '누', '군가', '를', '찾', '아', '헤매', '는', '사람', '이', '오', '는', '이벤트', '가', '확률', '적', '으로', '발생', '하', 'ㄴ다', '.', '노숙자', '는', '죽이', '었', '을', '때', '얻', '을', '수', '있', '는', '아이템', '이', '가치', '가', '높', '지', '않', '기', '때문', '에', '되', '도록', '돕', '는', '것', '이', '낫', '다', '.', '간혹', '노숙자', '에게', '음식', '을', '주', '었', '을', '때', ',', '노숙자', '가', '감사', '인사', '와', '함께', '따라오', '라고', '하', '는데', ',', '이', '때', '따라가', '면', '중간', '에', '멈추', '고', '혼자', '돌아가', '는', '경우', '가', '있', '는데', ',', '버', '그', '로', '추정', '되', 'ㄴ다', '.', '식량', '과', '의약품', '은', '없', '지만', '각종', '재료', '와', '부품', '이', '매우', '많이', '소장', '되', '어', '있', '다', '.', '보리스', ',', '마르코', '와', '같이', '수집', '에', '능하', 'ㄴ', '생존자', '를', '활용', '하', '아', '몇', '번', '씩', '방문', '하', '면서', '최대한', '많', '은', '양', '을', '확보', '하', '는', '것', '이', '좋', '다', '.', '반면', '식량', ',', '의약품', '등', '을', '구하', '기', '위하', '아', '약탈', '에', '의존', '하', '아야', '하', '는', '다소', '안정', '성', '이', '떨어지', '는', '조합', '의', '경우', '크', 'ㄴ', '이득', '을', '가져오', '지', '는', '못하', 'ㄴ다', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8000it [01:09, 115.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgwah5hB9mPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ab513ef-976d-4982-d3cc-2dabe585d251"
      },
      "source": [
        "print(\"Total tokens :\", len(vocab_freq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tokens : 71102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNtr94s9xcAA"
      },
      "source": [
        "#### Step 2. 단어의 Frequency에 따라 단어사전에 포함할 토큰 골라내기\n",
        "👉학습 코퍼스에는 총 70000개 이상의 토큰이 포함되어 있네요.\n",
        "\n",
        "코퍼스 자체가 크지 않다보니 이 토큰을 모두 포함해 단어사전을 만들어도 무리는 없는 숫자입니다.   \n",
        "하지만 연습을 위해, 이 중 70000개 토큰만을 추려서 사전을 만들겠습니다.   \n",
        "Counter의 most_common 기능을 이용해서 가장 많이 등장한 토큰과 가장 적게 등장한 토큰을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elE87f1H9mfW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "dc8dbc42-5ac8-4d55-f12a-70d3c162def8"
      },
      "source": [
        "N = 70000\n",
        "most_common = vocab_freq.most_common(len(vocab_freq))\n",
        "\n",
        "print(\"가장 많이 나온 10개 토큰:\", most_common[:10])\n",
        "print(\"가장 적게 나온 10개 토큰:\",most_common[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "가장 많이 나온 10개 토큰: [('하', 85964), ('이', 62446), ('.', 58144), ('의', 51015), ('는', 50509), ('을', 49238), ('다', 48650), ('ㄴ', 45041), ('에', 44015), (',', 42909)]\n",
            "가장 적게 나온 10개 토큰: [('카리브 판', 1), ('아루바', 1), ('연방 속지', 1), ('수리남', 1), ('에콰도르령', 1), ('갈라파고스 제도', 1), ('칠레령', 1), ('로빈슨 크루소', 1), ('칠로에 섬', 1), ('티에라델푸에고 제도', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWEV5p76yKgR"
      },
      "source": [
        "👉조사나 마침표 같은 토큰이 가장 많이 등장했고, 한 두번씩만 등장한 고유명사들이 가장 적게 나온 토큰으로 꼽혔습니다.\n",
        "\n",
        "\n",
        "이제 가장 자주 등장한 N=70000개 토큰만을 포함해 vocabulary_set을 만들겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gbovYZ6yFsS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "706c6032-c10e-441f-ab4a-1d051b763057"
      },
      "source": [
        "vocabulary_set = [s[0] for s in most_common[:N]]\n",
        "print(vocabulary_set[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['하', '이', '.', '의', '는', '을', '다', 'ㄴ', '에', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVNf5fL3ydiV"
      },
      "source": [
        "최종 단어사전은 위에서 골라낸 토큰과 더불어 [PAD]와 [UNK] 토큰을 포함해야겠지요?\n",
        "\n",
        "딥러닝에서 [PAD]토큰은 0번 인덱스를 사용하는 것이 일반적입니다.   \n",
        "\n",
        "따라서 0번째 자리에 [PAD], 1번째에 [UNK],    \n",
        "2번째부터는 위에서 골라낸 vocabulary_set에 있는 단어들이 차례로 오는 vocabulary_list를 만들겠습니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6r9LGJYksl_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7b1ce0c-ba71-4bf1-efff-006ddbf71126"
      },
      "source": [
        "## 단어 사전 생성을 위해 [PAD] , [UNK] 토큰을 포함하는 vocabulary_list 생성\n",
        "vocabulary_list = [\"[PAD]\", \"[UNK]\"]\n",
        "vocabulary_list.extend(vocabulary_set)\n",
        "\n",
        "print(\"최종 단어 개수: {}개\".format(len(vocabulary_list)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 단어 개수: 70002개\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD-WPqPzqU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c6e910b-31f1-4d11-9900-5c741c20f68c"
      },
      "source": [
        "vocabulary_list[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]', '[UNK]', '하', '이', '.', '의', '는', '을', '다', 'ㄴ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ72xt9_1Bmq"
      },
      "source": [
        "#### Step 3. 만들어진 단어사전 저장하기\n",
        "\n",
        "👉[PAD]와 [UNK] 토큰을 포함해 총 70002개의 토큰을 포함한 단어사전이 완성되었습니다.   \n",
        "이제 이 사전을 구글드라이브에 저장하여 다음 모듈에서 활용하도록 하겠습니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiZpb_IU1BHQ"
      },
      "source": [
        "import io\n",
        "out_m = io.open(\"/content/gdrive/My Drive/NLP/meta.tsv\" , 'w', encoding='utf-8')\n",
        "for num, word in enumerate(vocabulary_list):\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}