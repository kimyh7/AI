{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_BERT_토크나이저_사용하기_정답.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyLlLw0muJK4"
      },
      "source": [
        "# BERT 토크나이저 실습\n",
        "\n",
        "이번 실습에서는 구글에서 공개한 Multi-lingual BERT를 다운로드해 사용해보겠습니다.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFKvHZ6zmql8"
      },
      "source": [
        "## #1. 필요한 라이브러리 설치 및 로딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg7SJ1s6mxvA"
      },
      "source": [
        "#### ▶ pip로 bert-for-tf2 설치하기\n",
        "\n",
        "bert-for-tf2 패키지를 사용하면 BERT tokenizer을 아주 쉽게 사용할 수 있습니다.    \n",
        "<font color=\"blue\"> 정상적인 코드 실행을 위해 아래 konlpy 설치 명령어 실행 후 [런타임]-[런타임 다시시작]을 클릭해주세요\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUI6zuUdTyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "967e3c3e-c32d-4fe4-d569-8c4d75585d33"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/c1/015648a2186b25c6de79d15bec40d3d946fcf1dd5067d1c1b28009506486/bert-for-tf2-0.14.6.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.7MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.6-cp36-none-any.whl size=30318 sha256=e5a2de3aae5275c04de615b3485509f585e0fe34ebfc8a4c62cbc196f8a7d998\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/a0/b4/75b0601ebaa41e517a797fe9cea119c789664c8408f8a74ae9\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=1a2846250fafa29e602b15157c38914e576e73219701fbbef48b520f30639bca\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=d9cab1f7dfb3486dc513e6bcc0822af4ae46c73d649f32b6a56c8a35ee569169\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.6 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.6MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 29.0MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, tweepy, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZQqipinJc-"
      },
      "source": [
        "#### ▶ 필요한 라이브러리 로딩\n",
        "방금 설치한 bert 패키지와 TensorFlow Hub를 로딩하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISF7wYaEnVdW"
      },
      "source": [
        "## bert 모듈 로딩 & TF hub 연결\n",
        "\n",
        "import bert\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6TuaxlQwiob"
      },
      "source": [
        "## #2. 사전학습된 BERT 모델 로딩\n",
        "Tensorflow hub에서 pretrain된 다국어 BERT 모델을 가져오는 코드입니다.   \n",
        "홈페이지에서 Multi-lingula BERT에 해당하는 주소를 복사해 BERT_MODEL_HUB에 입력했습니다. \n",
        "\n",
        "그리고 hub.KerasLayer 함수를 이용해 bert_layer를 가지고 왔습니다.   \n",
        "이 레이어가 바로 Transformer 인코더가 12층 쌓여있는 BERT 모델입니다.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcbF_WliIPs"
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\n",
        "\n",
        "# BERT layer 가져오기\n",
        "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hktrgEmu3pz"
      },
      "source": [
        "## #3. BERT parsing 이해하기\n",
        "- BERT에서는 Wordpiece Tokenization을 통해 토큰을 subtoken으로 쪼갭니다.    \n",
        "- 한국어의 경우 원형을 보존하는 형태소 분석을 거친 후 subtoken으로 쪼개는 것이 좋습니다.   \n",
        "- 위에서 로딩한 bert_layer에서 사전학습에 활용한 토크나이저를 로딩할 수 있습니다.  \n",
        "   - bert.tokenization.bert_tokenization 함수 사용\n",
        "   - 형태소 분석기를 이용해 문장을 형태소 단위로 쪼갠 후\n",
        "   - tokenizer의 <font color=\"blue\">FullTokenizer</font> 을 사용해 Sub-tokenization 진행\n",
        "\n",
        "- 우리가 Python으로 코딩했던 @convert_tokens_to_ids나 @convert_ids_to_tokens매서드가 bert 패키지에 모두 포함되어 있습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSu5oGnxwqgd"
      },
      "source": [
        "#### Step 1. 토크나이저 로딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSbSQ10uj7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6f1661d4-5a8f-47f1-ed2f-49925755dcf7"
      },
      "source": [
        "from  bert.tokenization import bert_tokenization\n",
        "\n",
        "# vocab_file 가져오기\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "# 소문자화를 하는지 여부 가져오기\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "# 토크나이저 로딩\n",
        "print(\"vocab file:\", vocab_file)\n",
        "print(\"do_lower_case:\", do_lower_case)\n",
        "\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab file: b'/tmp/tfhub_modules/3e9209b9f2a53dfa4e6d93250dfceb5e64d73b66/assets/vocab.txt'\n",
            "do_lower_case: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asdN3KjDrNZE"
      },
      "source": [
        "👉 BERT에서 사용하는 단어사전이 위에 프린트된 경로에 txt 파일로 저장되어 있습니다.   \n",
        "👉 소문자화를 진행하여 학습한 BERT도 있지만 다국어 모델은 소문자화를 하지 않았기 때문에 do_lower_case=False인 것을 볼 수 있습니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42k2GhFMvwan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "de163e9c-a625-4752-c158-a995aa83a7b3"
      },
      "source": [
        "# vocab 사전 확인하기\n",
        "\n",
        "print(\"단어사전에 있는 토큰 개수:\", len(tokenizer.vocab))\n",
        "print(\"예시:\", list(tokenizer.vocab.keys())[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어사전에 있는 토큰 개수: 119547\n",
            "예시: ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<S>', '<T>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '°', '±', '²', '³', 'µ', '¶', '·', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ú', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sp8PwK7rpJR"
      },
      "source": [
        "👉 총 119,547개의 토큰이 포함되어 있습니다.   \n",
        "👉 [PAD] 토큰부터 시작해 unused로 예약된 자리가 있고, 영어, 러시아어(?) 등 다양한 언어의 토큰들이 포함되어 있습니다.   \n",
        "👉 한국어만을 위한 모델이 아니기 때문에, 한국어만으로 사전학습한 BERT에 비해서는 성능이 떨어집니다 ㅠ.ㅠ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCkpLouDmAJr"
      },
      "source": [
        "\"\"\" 형태소 분석 함수 \"\"\"\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "def tokenize(lines):\n",
        "  return okt.morphs(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o089Zp2-Ijp"
      },
      "source": [
        "👉 1~2일차 실습에서 저희는 Komoran 형태소분석기를 사용했습니다.    \n",
        "하지만 Komoran은 원형을 복원하여 형태소를 분석하는 아이였습니다.   \n",
        "\n",
        "👉 Sub-tokenizing을 위해서는 문장을 그대로 쪼개기만 해야 하기 때문에, okt 분석기를 사용하였습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxT95BwawwXU"
      },
      "source": [
        "#### Step 2. 형태소 분석 + Subtokenization 실행하기\n",
        "- tokenizer.tokenize 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i20ZPyEMQJYl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "75e129b7-e07e-41ef-b264-018364924f8e"
      },
      "source": [
        "sentence = \"버트로 토크나이즈하는 예제\"\n",
        "\n",
        "# basic_tokenizer로 문장 쪼개기\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "# BPE로 문장 쪼개기\n",
        "sub_tokens = tokenizer.tokenize(\" \".join(tokenized_sentence))\n",
        "print(sub_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['버트', '로', '토크', '나', '이즈', '하', '는', '예제']\n",
            "['버', '##트', '로', '토', '##크', '나', '이', '##즈', '하', '는', '예', '##제']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OZPX2p9uzjd"
      },
      "source": [
        "👉 tokenizer.tokenize 함수를 통해 형태소 분석된 문장을 WordPiece 단위로 쪼갭니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHWQn07B4Oq5"
      },
      "source": [
        "🙆‍♀️ 원하는 자연어 문장을 BERT tokenizer로 쪼개고 결과를 확인해 보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghjyqZN4M8g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "87f4dfd6-523e-49d6-c9a8-8adfee665129"
      },
      "source": [
        "sentence = \"햄버거는 역시 버거킹\"\n",
        "\n",
        "# basic_tokenizer로 문장 쪼개기\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "# Sub-token으로 쪼개기\n",
        "print(tokenizer.tokenize(\" \".join(tokenized_sentence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['햄버거', '는', '역시', '버거킹']\n",
            "['햄', '##버', '##거', '는', '역시', '버', '##거', '##킹']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOwbFPOwzIU"
      },
      "source": [
        "#### Step 3. BPE 토큰을 모델 인풋 인덱스로 바꾸기\n",
        "- tokenizer.convert_tokens_to_ids를 사용하면 Subtoken을 인덱스로 바꿀 수 있습니다. \n",
        "- 우리가 코딩해서 사용했던 방식과 동일하게 작동합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGhw8GKGQJUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "37874a21-70df-48e0-99cb-c8a06ced717e"
      },
      "source": [
        "# 모델 인풋 인덱스로 바꾸기\n",
        "print(sub_tokens)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['버', '##트', '로', '토', '##크', '나', '이', '##즈', '하', '는', '예', '##제']\n",
            "[9336, 15184, 9202, 9873, 20308, 8982, 9638, 24891, 9952, 9043, 9576, 17730]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQFbO9yy4MAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62c5b633-ec52-4e84-997f-1c67765518eb"
      },
      "source": [
        "# 인풋 인덱스를 토큰으로 바꾸기\n",
        "reversed_token = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['버', '##트', '로', '토', '##크', '나', '이', '##즈', '하', '는', '예', '##제']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwdOTe6ovOgE"
      },
      "source": [
        "## #4. BERT vocab 커스터마이즈하기\n",
        "BERT에는 무려 99개의 unused 토큰 자리가 예약되어 있습니다.   \n",
        "이 자리를 어떤 식으로 활용할 수 있을까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTr0ZzG-vcoy"
      },
      "source": [
        "먼저 원래 단어사전 text 파일을 열어 org_vocabs라는 리스트에 읽어오겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRf8Y8qiGlUX"
      },
      "source": [
        "## 원래 단어 사전 확인하기\n",
        "\n",
        "with open(vocab_file) as f:\n",
        "  org_vocabs = [s.strip() for s in f.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJpoq_a3GlOp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "db5080d4-91b6-4d34-b2ba-bd8da1bc85c5"
      },
      "source": [
        "print(\"# vocabs:\", len(org_vocabs))\n",
        "print(org_vocabs[:101])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# vocabs: 119547\n",
            "['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmRe37uGOxKX"
      },
      "source": [
        "이번 프로젝트로 LG CNS 블로그 댓글에 대한 감성 모니터링 과제를 수행하려고 합니다.   \n",
        "자사의 블로그이다보니 \\<CNS>, <엘지> 같은 단어들이 많이 보입니다.   \n",
        "\n",
        "저희 회사 이름이 들어간 만큼 이 토큰들은 subword로 토크나이즈되는 대신 하나의 의미 단위로 분석하고 싶은데요,   \n",
        "먼저 원래 BERT 단어사전에 이 단어들이 포함되어 있는지 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5g5Gu7YGlLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4913959d-a0c0-4adc-d074-92bad0f2b2d7"
      },
      "source": [
        "print(\"CNS\" in org_vocabs)\n",
        "print(\"엘지\" in org_vocabs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A7hHqZawGUd"
      },
      "source": [
        "👉 이런, 구글이 공개한 BERT의 단어사전에는 이 토큰들이 포함되어 있지 않습니다.   \n",
        "👉 그렇다면 지금은 이런 토큰들이 포함된 문장은 어떻게 파싱되고 있는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGRwcYo5IIU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "470449e3-cf8b-4d5e-a61f-f8982d363a81"
      },
      "source": [
        "tokenized = tokenizer.tokenize(\"안녕하세요 엘지 CNS 임승영 선임 연구원입니다.\")\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print(input_ids)\n",
        "reversed_token = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9521, 118741, 35506, 24982, 48549, 9562, 12508, 73067, 10731, 9644, 48210, 30858, 9428, 36240, 91785, 14279, 58303, 48345, 119]\n",
            "['안', '##녕', '##하', '##세', '##요', '엘', '##지', 'CN', '##S', '임', '##승', '##영', '선', '##임', '연구', '##원', '##입', '##니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbQe1R5bwX-T"
      },
      "source": [
        "👉 Vocab에 단어가 없다보니 CNS는 CN ##S , 엘지는 엘 ##지 로 찢어져서 토크나이징되고 있습니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fP7sRx1wx2Y"
      },
      "source": [
        "이런 현상을 방지하기 위해, 분리되지 않고 분석되었으면 하는 토큰들을 추가해 새로운 단어사전을 만들고   \n",
        "이를 텍스트 파일로 저장하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxRXN9KpJHgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "02231895-0690-4f10-f955-47f8c84d5ded"
      },
      "source": [
        "never_split = [\"엘지\", \"CNS\"]\n",
        "\n",
        "## 추가한 never_split 단어를 반영해 새로운 사전을 만들어주기\n",
        "new_vocabs = org_vocabs.copy()\n",
        "idx = 1\n",
        "for tok in never_split:\n",
        "  if tok not in org_vocabs: # (안전장치 1) 원래 vocab에 없으면\n",
        "    if \"unused\" in new_vocabs[idx]: # (안전장치 2) [unused] 토큰 자리이면\n",
        "      new_vocabs[idx] = tok\n",
        "      print(\"{} -> {}\".format(org_vocabs[idx], new_vocabs[idx]))\n",
        "      idx += 1\n",
        "    else:\n",
        "      \"Cannot Allocate New Token Anymore\"\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[unused1] -> 엘지\n",
            "[unused2] -> CNS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nbDGdKaw9-I"
      },
      "source": [
        "👉 new_vocabs에는 never_split으로 정한 토큰들을 포함한 단어 리스트가 저장됩니다. \n",
        "\n",
        "새로운 단어를 추가하는 과정에서는 두 가지 안전장치를 넣어주었습니다. \n",
        "1. 원래 vocab에 없는 경우에만 추가하기 -> 단어사전에는 중복이 있으면 안 되기 때문입니다.   \n",
        "2. [unused #] 토큰일 때만 대체하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqGmm2aGqMyj"
      },
      "source": [
        "## 새 단어사전 저장\n",
        "new_vocab_file = \"/content/new_vocab.txt\"\n",
        "\n",
        "with open(new_vocab_file, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_vocabs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-4vcsdOxz6T"
      },
      "source": [
        "이제 새로 저장한 단어사전을 사용해 new_tokenizer라는 이름으로 다시 한 번 토크나이저를 로딩하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeYyFCbcqSm0"
      },
      "source": [
        "## 새로운 사전을 이용해 로딩\n",
        "\n",
        "new_tokenizer = bert_tokenization.FullTokenizer(new_vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDvr0xwVJQs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0ce30034-e0c1-493f-d4f0-2e8ea5ccae70"
      },
      "source": [
        "tokenized = new_tokenizer.tokenize(\"안녕하세요 엘지 CNS 임승영 선임 연구원입니다.\")\n",
        "print(tokenized)\n",
        "input_ids = new_tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print(input_ids)\n",
        "reversed_token = new_tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['안', '##녕', '##하', '##세', '##요', '엘지', 'CNS', '임', '##승', '##영', '선', '##임', '연구', '##원', '##입', '##니다', '.']\n",
            "[9521, 118741, 35506, 24982, 48549, 1, 2, 9644, 48210, 30858, 9428, 36240, 91785, 14279, 58303, 48345, 119]\n",
            "['안', '##녕', '##하', '##세', '##요', '엘지', 'CNS', '임', '##승', '##영', '선', '##임', '연구', '##원', '##입', '##니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEN2AfLyWMA"
      },
      "source": [
        "👉 토큰을 추가했기 때문에 이번에는 \"엘지\"와 \"CNS\"가 쪼개지지 않고 토크나이징되었습니다.   \n",
        "👉 블로그 댓글에 대한 학습 데이터를 학습하는 과정에서 BERT는 fine-tuning을 통해 새로 추가된 텍스트의 의미를 학습하게 될 것입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKyRlIK8ynsQ"
      },
      "source": [
        "## #5. DAILY MISSION   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt693N4gzFSL"
      },
      "source": [
        "<font color=\"red\">MISSION: BERT 토크나이징 & 인덱싱 해보기 </font>\n",
        "\n",
        "BERT를 사용해 감성분석 과제를 수행하고자 합니다.    \n",
        "감성분석을 수행하기 위해서는 인풋 문장을 <b>[CLS] 인풋문장 [SEP]</b>의 형태로 만들어야 합니다. \n",
        "\n",
        "인풋 문장으로 \"BERT 알고 보니 완전 쉽네\"라는 문장이 들어왔습니다.   \n",
        "\n",
        "1. 형태소분석과 BERT 토크나이징을 진행해 위의 문장을 subtokenize하고,    \n",
        "[CLS] 인풋문장 토큰들 [SEP] 의 형태로 만드세요. \n",
        "2. 토크나이즈된 문장을 BERT의 단어사전을 사용해 정수 인덱스로 변환하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWWDkiFe4CGp"
      },
      "source": [
        "<실행 결과는 예시>   \n",
        "- 형태소 분석 후 -> ['BERT', '알', '고', '보니', '완전', '쉽네']\n",
        "- Sub-tokenizing 후 -> ['BE', '##RT', '알', '고', '보', '##니', '완', '##전', '쉽', '##네']\n",
        "- BERT 인풋 형태 변환 -> ['[CLS]', 'BE', '##RT', '알', '고', '보', '##니', '완', '##전', '쉽', '##네', '[SEP]']\n",
        "- BERT 정수 인덱스 변환 -> [101, 46291, 46935, 9524, 8888, 9356, 25503, 9591, 16617, 9471, 77884, 102]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775USuqpItyW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "afa4ba50-405b-4a3a-a5a0-193f745b7bab"
      },
      "source": [
        "\"\"\" Your Code Here \"\"\"\n",
        "\n",
        "sentence = \"BERT 알고 보니 완전 쉽네\"\n",
        " \n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        " \n",
        "# Sub-token으로 쪼개기\n",
        "sub_tokenized_sent = tokenizer.tokenize(\" \".join(tokenized_sentence))\n",
        "print(sub_tokenized_sent)\n",
        " \n",
        "# [CLS] Subtokens [SEP] 형태로 만들기\n",
        "sub_tokenized_sent = [\"[CLS]\"] + sub_tokenized_sent + [\"[SEP]\"]\n",
        "print(sub_tokenized_sent)\n",
        " \n",
        "# 정수 인덱스로 변환하기\n",
        "input_ids = tokenizer.convert_tokens_to_ids(sub_tokenized_sent)\n",
        "print(input_ids)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['BERT', '알', '고', '보니', '완전', '쉽네']\n",
            "['BE', '##RT', '알', '고', '보', '##니', '완', '##전', '쉽', '##네']\n",
            "['[CLS]', 'BE', '##RT', '알', '고', '보', '##니', '완', '##전', '쉽', '##네', '[SEP]']\n",
            "[101, 46291, 46935, 9524, 8888, 9356, 25503, 9591, 16617, 9471, 77884, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}