{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_3_1_RNN_감성분석_데이터전처리_정답.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "# 실습 3. RNN을 이용한 😀감정분석😑 모델 학습하기\n",
        "\n",
        "\n",
        "\n",
        "<b>학습 목표:    \n",
        "- NLU 모델링을 위한 데이터 전처리(토크나이징 & 인코딩, 라벨 만들기)를 이해하고 Python을 사용해 코딩한다.\n",
        "</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5VEKjCVzQz"
      },
      "source": [
        "## #0. 실습 준비하기\n",
        "먼저 구글 드라이브를 마운트하고, 필요한 라이브러리를 설치하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFVQxGin7GQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cg9V5xCV-YJ",
        "outputId": "7d2ef5be-f439-45fc-ec63-489d6769c4e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ695haKKAwv",
        "outputId": "c913cfdb-78be-48b7-a9b0-27a1d2534af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "\"\"\" 한국어 형태소 분석 라이브러리 \"\"\"\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.1MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, tweepy, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7-K8wDgPMKt",
        "outputId": "26f71f35-f0f9-4a38-8ac8-a8bcda60468e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQxrMxDHXEqF"
      },
      "source": [
        "깃허브에 있는 감성분석 데이터셋을 다운로드해 읽어오겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVmEgziXKav",
        "outputId": "76a477fe-eacc-41da-f376-82d9c21abdc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-06 07:39:48--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14628807 (14M) [text/plain]\n",
            "Saving to: ‘ratings_train.txt’\n",
            "\n",
            "ratings_train.txt   100%[===================>]  13.95M  30.2MB/s    in 0.5s    \n",
            "\n",
            "2020-10-06 07:39:49 (30.2 MB/s) - ‘ratings_train.txt’ saved [14628807/14628807]\n",
            "\n",
            "--2020-10-06 07:39:49--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4893335 (4.7M) [text/plain]\n",
            "Saving to: ‘ratings_test.txt’\n",
            "\n",
            "ratings_test.txt    100%[===================>]   4.67M  13.8MB/s    in 0.3s    \n",
            "\n",
            "2020-10-06 07:39:50 (13.8 MB/s) - ‘ratings_test.txt’ saved [4893335/4893335]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7QYyS8iOnvN",
        "outputId": "265c2464-75ec-44f5-a301-60860c39ccdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "  raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "  raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"긍정\"])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TEST.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TEST.append([line[0], line[1], \"긍정\"])\n",
        "\n",
        "def print_stat(name, data):\n",
        "  print(\"{:<10}: {}개 (긍정 {}, 부정 {})\".format(name, len(data), len([t for t in data if t[2]==\"긍정\"]), len(data)-len([t for t in data if t[2]==\"긍정\"])))\n",
        "\n",
        "print_stat(\"FULL_TRAIN\", FULL_TRAIN)\n",
        "print_stat(\"FULL_TEST\", FULL_TEST)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL_TRAIN: 150000개 (긍정 74827, 부정 75173)\n",
            "FULL_TEST : 50000개 (긍정 25173, 부정 24827)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkZC-6UY_gZl",
        "outputId": "1fb57418-8004-4d12-8b40-d574bc2c1efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# 데이터 예시 : id, 문장, 라벨 순서\n",
        "FULL_TRAIN[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '부정'],\n",
              " ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '긍정'],\n",
              " ['10265843', '너무재밓었다그래서보는것을추천한다', '부정'],\n",
              " ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '부정'],\n",
              " ['6483659',\n",
              "  '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
              "  '긍정'],\n",
              " ['5403919', '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', '부정'],\n",
              " ['7797314', '원작의 긴장감을 제대로 살려내지못했다.', '부정'],\n",
              " ['9443947',\n",
              "  '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네',\n",
              "  '부정'],\n",
              " ['7156791', '액션이 없는데도 재미 있는 몇안되는 영화', '긍정'],\n",
              " ['5912145', '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', '긍정']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ozOH9zi-pj"
      },
      "source": [
        "이제 데이터를 train / validation / test 셋으로 나누겠습니다.   \n",
        "학습 시간관계상 train 50000건, validation 10000건, test 10000건만 샘플링해서 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riLyUYubXciI",
        "outputId": "431ccb88-903e-438c-e6da-586c169ed21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "\n",
        "print_stat(\"train\", train)\n",
        "print(\".. ex:\", train[0])\n",
        "print_stat(\"validation\", val)\n",
        "print(\".. ex:\", val[0])\n",
        "print_stat(\"test\", test)\n",
        "print(\".. ex:\", test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train     : 50000개 (긍정 24914, 부정 25086)\n",
            ".. ex: ['7570203', '장쯔이 그때나 지금이나 이뻤다', '긍정']\n",
            "validation: 10000개 (긍정 4962, 부정 5038)\n",
            ".. ex: ['8413570', '거지 발싸개같은 영화도 자막 파일이 넘쳐나는게 허다한데, 왜 이 좋은 영화는 자막을 구할수가 없을까.. 60, 70년대의 프랑스, 이탈리아, 독일의 수작들이 자막이 없어서 수OO이 지난 지금도 일반 대중에게 제대로 알려지지 못했다.', '긍정']\n",
            "test      : 10000개 (긍정 5027, 부정 4973)\n",
            ".. ex: ['1458790', '허우 샤오시엔 작품은 모두 만점!', '긍정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLpeP2F3TI7p"
      },
      "source": [
        "## 나누어 놓은 train/ validation/ test 데이터 저장하기\n",
        "import json\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_train.json\", \"w\") as f:\n",
        "  f.write(json.dumps(train))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_val.json\", \"w\") as f:\n",
        "  f.write(json.dumps(val))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_test.json\", \"w\") as f:\n",
        "  f.write(json.dumps(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS"
      },
      "source": [
        "# #1. 토크나이징\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step1.PNG?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiAtd18qhb9t"
      },
      "source": [
        "먼저 Komoran 형태소분석기를 사용해 형태소 분석 함수 tokenize를 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8uFroJgKwi",
        "outputId": "9e796dfa-3447-4ed1-a2e1-87b962e4fd6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return komoran.morphs(sentence)\n",
        "\n",
        "tokenize(\"미션 완료!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미션', '완료', '!!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1JEphdYwyU"
      },
      "source": [
        "# #2. 단어 사전 로딩 & 인코딩\n",
        "이제 인코딩을 위해 단어사전을 로딩하겠습니다.   \n",
        "실습 소개에서 말씀드렸던 것처럼 CBOW 학습에서 사용한 70002개의 토큰에    \n",
        "이번 태스크 수행에서 새롭게 나온 토큰을 추가하겠습니다.\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step2.PNG?raw=true\">   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-cDT132ciHp"
      },
      "source": [
        "#### Step 1. 단어사전 로딩하기\n",
        "- 기 학습된 단어 임베딩을 불러와 사용\n",
        "- 도메인 특화적인 단어로 [UNK]가 되는 토큰들을 식별해 단어 사전에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACfKwipLZEze",
        "outputId": "7dbfef35-7f59-4a3a-98fa-a09297874bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "\"\"\" 지난 실슬 때 만들었던 단어 사전 로딩 \"\"\"\n",
        "\n",
        "## final_embeddings: 70002개 토큰에 대한 워드 벡터 매트릭스 shape=(70002, 128)\n",
        "## vocab_list: 위의 워드 벡터와 매칭되는 단어 사전\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/vecs.tsv\") as f:\n",
        "  vecs = [v.strip() for v in f.readlines()]\n",
        "final_embeddings = [v.split(\"\\t\") for v in vecs]\n",
        "final_embeddings = np.array(final_embeddings, dtype=\"float32\")\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/meta.tsv\") as f:\n",
        "  vocab_list = [v.strip() for v in f.readlines()]\n",
        "\n",
        "print(\"** 단어 사전 개수:\", len(vocab_list))\n",
        "print(\"** 단어벡터 예시: \")\n",
        "print(\"token :\", vocab_list[999])\n",
        "print(\"vector:\", final_embeddings[999])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 단어 사전 개수: 70002\n",
            "** 단어벡터 예시: \n",
            "token : 약속\n",
            "vector: [ 1.39787421e-01 -3.84234697e-01 -5.40808678e-01  1.47639737e-02\n",
            " -1.44524083e-01  7.03318939e-02  2.15433896e-01  3.47744254e-03\n",
            " -1.11955218e-02  4.17765081e-01  2.20791101e-01  4.46164668e-01\n",
            " -5.74833632e-01 -4.71866518e-01 -4.32424575e-01  2.22987652e-01\n",
            "  1.94115311e-01  2.63385952e-01  3.28183919e-01 -4.22089607e-01\n",
            " -1.04542814e-01  1.62697881e-01 -1.58821136e-01 -3.23788792e-01\n",
            " -5.71838796e-01  5.85799575e-01 -7.10058093e-01  1.43308431e-01\n",
            " -4.11221199e-02  2.94368807e-02  2.57899791e-01 -4.58910733e-01\n",
            "  1.42146766e-01 -3.38556468e-01  1.25622004e-01 -1.85091704e-01\n",
            "  4.02939171e-01 -1.01323523e-01  2.85541564e-01  2.12628245e-01\n",
            " -4.62328009e-02  1.20681420e-01 -1.23937130e-01  1.05506694e-02\n",
            "  3.15133363e-01 -2.08656356e-01  7.68174082e-02 -1.84903651e-01\n",
            "  1.83630511e-01 -2.24744290e-01  4.72059786e-01  4.76261348e-01\n",
            "  2.46252075e-01  2.08968930e-02  1.21434242e-01  3.05238903e-01\n",
            "  9.10391510e-01 -5.51669121e-01  3.25826526e-01 -1.11191712e-01\n",
            "  9.11423489e-02 -3.87720093e-02 -5.88440597e-01 -4.49651033e-02\n",
            " -2.96554744e-01 -2.87878644e-02  4.88854609e-02 -1.88614056e-01\n",
            "  2.03368872e-01  3.65248799e-01  1.89014420e-01 -8.72343257e-02\n",
            "  2.95743465e-01  5.49653053e-01 -7.07075000e-01  1.90146580e-01\n",
            " -3.29838485e-01  1.48678064e-01 -3.21026355e-01 -1.73299909e-01\n",
            " -5.75484037e-02  4.02653277e-01 -2.12377787e-01  1.69643387e-01\n",
            " -4.21411157e-01  2.17227731e-04  2.40858242e-01  1.22375682e-01\n",
            " -5.80040216e-02 -1.71981037e-01 -1.63062230e-01  4.59325522e-01\n",
            "  9.92589816e-02 -8.94955575e-01 -3.18319947e-01  4.78936017e-01\n",
            " -1.97972998e-01  2.67273366e-01  1.31765291e-01  2.07759812e-01\n",
            " -4.34342802e-01 -1.72573060e-01  1.39932930e-01 -6.92537665e-01\n",
            " -3.66498500e-01 -1.67859107e-01 -2.64989465e-01  4.21264060e-02\n",
            " -3.77243608e-01  3.08380097e-01  8.93778130e-02 -2.68326372e-01\n",
            "  2.40582705e-01  3.04847181e-01  2.12085876e-03  1.44711554e-01\n",
            " -1.98666900e-01 -1.27137534e-03 -2.05211222e-01  1.10412888e-01\n",
            "  1.45928487e-01  4.26627517e-01 -2.65018381e-02 -1.76784053e-01\n",
            " -2.14965958e-02  3.99546474e-01  9.45729390e-02  3.70869897e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHzLQxYSlWI"
      },
      "source": [
        "<font color=\"blue\"> 🙋‍♀️[QUIZ] final_embeddings의 차원이 70002 x 128인 이유는 무엇인가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDxTsjChDKs"
      },
      "source": [
        "현재 태스크에서 [UNK]로 떨어지는 단어가 어떤 것들이 있는지 살펴보겠습니다.   \n",
        "CBOW 실습에서 사용했던 collections의 Counter()기능을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IisJ8WUwZUV7",
        "outputId": "0c4129b9-0b88-4d16-b698-54cb545e87e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import collections \n",
        "from tqdm import tqdm\n",
        "tot_tokens = 0\n",
        "oov_counter = collections.Counter() #새로운 토큰 카운터\n",
        "\n",
        "Tokenized_train = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(train):\n",
        "  sent = dat[1]\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  tot_tokens += len(tokenized_sent) #토큰 개수\n",
        "  for word in tokenized_sent:\n",
        "    if word not in vocab_list: #기존 단어사전에서 찾을 수 없으면\n",
        "      oov_counter[word] += 1 #Counter에서 개수 세기\n",
        "      \n",
        "  Tokenized_train.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [02:00<00:00, 415.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8WfRvinZUh8",
        "outputId": "575c5a61-53d1-4020-f285-46d2c9af222d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(\"# OOV Tokens:\", len(oov_counter))\n",
        "print(\"{}/{} ({:.2f}%) are [UNK] in train tokens\".format(sum(oov_counter.values()) , tot_tokens , 100*sum(oov_counter.values())/tot_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# OOV Tokens: 15927\n",
            "36292/955855 (3.80%) are [UNK] in train tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbm16L8T3kz"
      },
      "source": [
        "👉 50,000개의 train 문장에서 무려 15,000개 이상의 Out-Of-Vocabulary 토큰들이 발견되었습니다.    \n",
        "🙋‍♀️ 어떤 토큰들이 기존 단어 사전에 포함되어 있지 않았을까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9O8yqJea7E6",
        "outputId": "cc8e88fc-dce4-439d-badf-2f4f9d7cd5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "most_common = oov_counter.most_common(len(oov_counter))\n",
        "print(most_common[:10])\n",
        "print(most_common[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ㅋㅋ', 647), ('......', 601), ('♥', 472), ('ㅋㅋㅋ', 358), ('ㅋ', 358), ('ㅠㅠ', 321), ('이쁘', 314), ('ㅎㅎ', 297), ('막장', 260), ('ㅡㅡ', 234)]\n",
            "[('스토리임....ㅡㅡ', 1), ('어렷을떄', 1), ('섬세하뮤ㅠ', 1), ('없슴다', 1), ('hhhh', 1), ('린날', 1), ('너거', 1), ('재밋었을듯..', 1), ('장음', 1), ('죽다ㅠ_ㅠ', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXz1rwmUMHH"
      },
      "source": [
        "예상했던 것과 같이 구어체적인 표현들이 새로 등장한 것을 볼 수 있습니다.   \n",
        "이제 새로 나온 토큰들을 기존의 단어사전에 추가해 감성분석 태스크 분석을 위한 새로운 단어사전을 만들겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdRpNRLafFu",
        "outputId": "c430880c-c527-4cfc-e71d-b774e4a11861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "### Train 데이터에서 새로 발견한 토큰을 기존 단어 사전에 추가\n",
        "new_vocab_list = vocab_list.copy() # 1. 기존 단어 사전 복사\n",
        "new_vocab_list.extend([v[0] for v in most_common]) # 2. 새로 나온 토큰을 기존 리스트에 이어붙이기\n",
        "print(\"# New Vocabs = {}\".format(len(new_vocab_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# New Vocabs = 85929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0m667zbUon4"
      },
      "source": [
        "<font color=\"blue\"> 🙋‍♀️[QUIZ] 새로 나온 토큰을 기존 리스트의 뒤에 이어붙여야 하는 이유는 무엇인가요?   \n",
        "🙋‍♀️[QUIZ] 새로운 단어사전은 몇 개의 토큰이 있나요? 모델에서 임베딩 차원은 몇 차원이 되어야 하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbgukIuZU7Lk"
      },
      "source": [
        "새로 만든 단어사전을 저장해 놓아야 이후 학습~추론에서 사용할 수 있겠지요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piDQCOt_0i5Z"
      },
      "source": [
        "## 새로 만든 단어사전 저장\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_vocab.json\", \"w\") as f:\n",
        "  f.write(json.dumps(new_vocab_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZZ2J9kUWqq_"
      },
      "source": [
        "#### Step 2. 데이터 토크나이징하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fGQbvXbVLcM"
      },
      "source": [
        "그럼 validation과 test 데이터에 대해서도 토크나이징을 진행하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWG3pA0p2e0L",
        "outputId": "372cce8d-4f6e-4ade-bbc5-f68de6e847d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "Tokenized_val = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(val):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_val.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장\n",
        "\n",
        "Tokenized_test = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(test):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_test.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1353.93it/s]\n",
            "100%|██████████| 10000/10000 [00:07<00:00, 1288.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avaO54uS2e5j",
        "outputId": "630c20c2-4afc-4501-a88e-fe4b1fb39a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(Tokenized_train) , len(Tokenized_val), len(Tokenized_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enZmxc4OWhTC"
      },
      "source": [
        "#### Step 3. 텍스트 인코더 코딩하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuO8hmMWHIR"
      },
      "source": [
        "토크나이징된 문장을 인덱스로 바꾸기 위해, 지난 모듈에서 정의했던 TextEncoder를 사용하겠습니다.    \n",
        "지난 시간에 코딩했던 내용을 utils.py에 담아두었습니다.   \n",
        "여기서 TextEncoder를 불러와 사용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a"
      },
      "source": [
        "from utils import TextEncoder\n",
        "text_encoder = TextEncoder(new_vocab_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1odlXGscJ1P",
        "outputId": "a8d03a50-a66b-4734-ba38-889ddb99554f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "## 토크나이저 테스트\n",
        "sent = \"새로 만든 단어사전 ♥\"\n",
        "tokenized_sent = tokenize(sent)\n",
        "tokenized_ids = text_encoder.convert_tokens_to_ids(tokenized_sent) # 토큰 -> 인덱스\n",
        "reversed_token= text_encoder.convert_ids_to_tokens(tokenized_ids) # 인덱스 -> 토큰\n",
        "\n",
        "print(tokenized_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1339, 128, 9, 1313, 1815, 70004]\n",
            "['새로', '만들', 'ㄴ', '단어', '사전', '♥']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8dBaSmcqCY"
      },
      "source": [
        "### Step 4. 태스크 수행을 위한 형태로 만들기   \n",
        "그럼 이제 본격적으로 감성분석 태스크를 수행할 수 있는 형태로 인풋을 준비하겠습니다.   \n",
        "\n",
        "이를 위해 아래의 세 가지를 수행해야 합니다.   \n",
        "1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)\n",
        "2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요   \n",
        "3. 배치 처리를 위해 패딩 & numpy array로 변환\n",
        "\n",
        "\n",
        "위의 세 가지 작업을 수행하는 함수로 create_cls_feature이라는 함수를 정의하겠습니다.    \n",
        "TextEncoder와 마찬가지로, 이 함수는 다양한 텍스트 분류 과제에서 코드 재활용이 가능합니다. \n",
        "\n",
        "create_cls_feature\n",
        "- 함수 인풋: 데이터 , text_encoder, max_seq_len\n",
        "- 함수 아웃풋: input_ids , labels, label_map   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5UdYcieZ7ef"
      },
      "source": [
        "그럼 데이터 example 일부에 대해 한 단계씩 차례대로 수행하며 차례대로 코딩해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWGM-lpgYblR"
      },
      "source": [
        "<b>1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)   </b>   \n",
        "함수는 인풋으로 Tokenized_train과 같은 데이터를 받습니다.   \n",
        "데이터는 리스트의 리스트로 이루어져 있으며, 각각의 리스트는 [문장 번호, 토큰화된 문장, 정답 라벨]의 모양으로 되어 있습니다.   \n",
        "(예) ['7570203', ['장쯔이', '그때', '나', '지금', '이나', '이뻤다'], '긍정']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFeFXFoYwlI",
        "outputId": "cf4da881-8db5-4c7c-e931-51ce45a1d759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "examples = Tokenized_train[:3]\n",
        "\n",
        "for example in examples:\n",
        "  print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['7570203', ['장쯔이', '그때', '나', '지금', '이나', '이뻤다'], '긍정']\n",
            "['6317334', ['음식', '가지', '고', '사람', '을', '해치', 'ㄴ', '사람', '이', '명장', '되', '고', ',', '인', '주', '는', '끝', '까지', '반성', '도', '없', '구', ',', '막장', '드라마', '로', '기억', '되', 'ㄹ', '듯'], '부정']\n",
            "['6287811', ['뭔내용인지도모르겟고', '돈', '아깝', '고'], '부정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjWcIQxqZPbo"
      },
      "source": [
        "<font color=\"red\"> [MISSION] examples에 있는 토큰화된 자연어 문장들을 인덱스로 변환해 input_ids라는 리스트에 저장해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfS-sNNIZO_H",
        "outputId": "07fb930f-3ab0-4be7-e487-202712854788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "input_ids = []\n",
        " \n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  input_id = text_encoder.convert_tokens_to_ids(tokenized_sent)  ## [★ CODE ★]\n",
        "  input_ids.append(input_id)\n",
        " \n",
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [34556, 1862, 75, 467, 150, 71299]\n",
            "문장 1: [2103, 110, 16, 111, 7, 12533, 9, 111, 3, 11094, 17, 16, 11, 435, 62, 6, 540, 61, 5730, 31, 58, 562, 11, 70010, 526, 24, 1478, 17, 32, 1036]\n",
            "문장 2: [73219, 762, 11267, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4igxqa1aFT8"
      },
      "source": [
        "<b>2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요    </b>   \n",
        "\n",
        "다음으로 모델에게 라벨을 알려줄 수 있는 라벨 매핑 사전이 필요합니다.    \n",
        "모델은 \"긍정\"이나 \"부정\" 이라는 단어를 인식할 수 있기 때문에, 각 라벨에 해당하는 정수 인덱스를 매핑해주는 것입니다.   \n",
        "{\"긍정\": 0 , \"부정\": 1} 이런 식으로요.   \n",
        "\n",
        "이 때 라벨 매핑 사전은 학습~추론에서 유지되어야 하겠지요?   \n",
        "따라서 학습 데이터를 이용해   \n",
        "1. 새로 발견된 라벨을 라벨 매핑 사전에 저장한다.\n",
        "2. 완성된 라벨 매핑 사전을 저장해둔다.   \n",
        "\n",
        "이후 검증/ 테스트 데이터에서는 위에서 만든 라벨 매핑 사전을 사용해 매핑만 해주도록 코드를 구현하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMwuDNZlb673",
        "outputId": "04d51efa-ed4d-4b7b-8287-5d91256a322f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "CREATE_LABEL_MAP = True # 새로 사전을 만들겠다. \n",
        "label_map = {}\n",
        "label_index = 0 \n",
        "\n",
        "label_ids = [] ## 데이터에 대해 변환된 라벨 저장\n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  if label not in label_map: # 라벨이 label_map에 없다면\n",
        "    if CREATE_LABEL_MAP:\n",
        "      label_map[label] = label_index # 라벨 맵에 해당 라벨 추가\n",
        "      label_index += 1\n",
        "      label_id = label_map[label]      \n",
        "    else:\n",
        "      ## train 이외의 단계에서 새로운 라벨이 발견되었다면, 이는 잘못된 데이터입니다. \n",
        "      ## 따라서 Error 메시지를 추가하고, 건너뛰도록 하겠습니다.\n",
        "      print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "      continue\n",
        "  else: # 라벨을 찾았으면\n",
        "    label_id = label_map[label]\n",
        "  label_ids.append(label_id)\n",
        "\n",
        "print(\"라벨 매핑 사전:\", label_map)\n",
        "print(\"-> 원래 라벨:\", [e[2] for e in examples])\n",
        "print(\"-> 인덱싱 후:\", label_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "라벨 매핑 사전: {'긍정': 0, '부정': 1}\n",
            "-> 원래 라벨: ['긍정', '부정', '부정']\n",
            "-> 인덱싱 후: [0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7-rRYFKda7k"
      },
      "source": [
        "<b>3. 배치 처리를 위해 패딩 & numpy array로 변환    </b>   \n",
        "\n",
        "정수 인덱스로 변환한 input_ids를 살펴볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27syNaIdpA1",
        "outputId": "bd4c2af0-2f21-4bb9-e09f-657a1b952ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [34556, 1862, 75, 467, 150, 71299]\n",
            "문장 1: [2103, 110, 16, 111, 7, 12533, 9, 111, 3, 11094, 17, 16, 11, 435, 62, 6, 540, 61, 5730, 31, 58, 562, 11, 70010, 526, 24, 1478, 17, 32, 1036]\n",
            "문장 2: [73219, 762, 11267, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkE_YPI_dqMG"
      },
      "source": [
        "👉자연어 문장은 길이가 제각각이기 때문에 각 문장의 길이가 제각각인 것을 볼 수 있습니다.   \n",
        "\n",
        "하지만 딥러닝 배치 처리를 위해서는 이들을 일정한 길이로 맞춰주어야 하지요.   \n",
        "이 기능을 수행해 주는 것이 tensorflow.keras.preprocessing.sequence의 pad_sequences 기능입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bARuY8gfYkDs",
        "outputId": "abf7e895-2235-480f-dbfb-8fff23dea5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_len = 20\n",
        "\n",
        "input_ids_1 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 뒤에 한다\n",
        "                            truncating=\"post\" ) # 최대 길이를 넘어갈 경우 뒷 부분을 자른다\n",
        "print(\"** 예시 1:\")\n",
        "print(input_ids_1)\n",
        "input_ids_2 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 앞에 한다\n",
        "                            truncating=\"pre\" ) # # 최대 길이를 넘어갈 경우 앞 부분을 자른다\n",
        "print(\"\\n** 예시 2:\")\n",
        "print(input_ids_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 예시 1:\n",
            "[[34556  1862    75   467   150 71299     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [ 2103   110    16   111     7 12533     9   111     3 11094    17    16\n",
            "     11   435    62     6   540    61  5730    31]\n",
            " [73219   762 11267    16     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n",
            "\n",
            "** 예시 2:\n",
            "[[34556  1862    75   467   150 71299     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   17    16    11   435    62     6   540    61  5730    31    58   562\n",
            "     11 70010   526    24  1478    17    32  1036]\n",
            " [73219   762 11267    16     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUeAbTSPYjdQ"
      },
      "source": [
        "<font color=\"red\"> [MISSION] 한국어에서 핵심적인 정보는 문장의 뒷 부분에 있다고 판단하였습니다.   \n",
        "따라서, 문장이 max_seq_len을 넘어가면 앞부분을 자르고, 모자르면 문장 뒤에 0 패딩을 하기로 결정하였습니다.   \n",
        "pad_sequences 함수를 사용해 이 기능을 수행할 수 있도록 직접 코딩해보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMIs9ehfAKe"
      },
      "source": [
        "input_ids = pad_sequences(input_ids,\n",
        "                            maxlen=max_seq_len,\n",
        "                            padding=\"post\",\n",
        "                            truncating=\"pre\")\n",
        " \n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uzYr10RfIYV"
      },
      "source": [
        "그럼 위에서 하나씩 만들었던 함수를 모두 합쳐서 create_cls_feature 함수를 만들겠습니다.   \n",
        "이 함수는 utils.py에도 저장되어 있어, 향후에도 텍스트 분석에서 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upowc1lDca9w"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def create_cls_feature(examples, text_encoder, max_seq_len, label_map=None):\n",
        "\n",
        "  input_ids = [] # 정수 인덱스로 변환한 문장들의 리스트\n",
        "  labels = [] # 정답 라벨 리스트\n",
        "\n",
        "  if label_map is None: #label_map이 없으면 -> 이번에 데이터를 처리하면서 새로 생성함\n",
        "    CREATE_LABEL_MAP = True\n",
        "    label_map = {}\n",
        "    label_index = 0\n",
        "  else: \n",
        "    CREATE_LABEL_MAP = False\n",
        "    print(\"** Start creating features using label map\")\n",
        "    print(label_map)\n",
        "\n",
        "  for example in examples:\n",
        "    idx, tokenized_sent, label = example\n",
        "\n",
        "    ## 1. text_encoder 사용해 정수로 변환 \n",
        "    input_id = text_encoder.convert_tokens_to_ids(tokenized_sent)\n",
        "    if len(input_id) == 0:\n",
        "      print(\"Sentence with length = 0... continue\", example)\n",
        "      continue\n",
        "\n",
        "    ## 2. label 매핑 & index로 변환\n",
        "    if label in label_map:\n",
        "      label_id = label_map[label]\n",
        "    else:\n",
        "      if CREATE_LABEL_MAP:\n",
        "        # label map에 추가\n",
        "        label_map[label] = label_index\n",
        "        label_index += 1\n",
        "        label_id = label_map[label]\n",
        "      else:\n",
        "        print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "        continue\n",
        "  \n",
        "    ## 전체 리스트에 append\n",
        "    input_ids.append(input_id)\n",
        "    labels.append(label_id)\n",
        "    \n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  input_ids = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, \n",
        "                            padding=\"post\", \n",
        "                            truncating=\"pre\")\n",
        "\n",
        "  ## np.array로 변환해 리턴\n",
        "  input_ids = np.array(input_ids)\n",
        "  labels = np.array(labels)\n",
        "  \n",
        "  assert len(input_ids) == len(labels)\n",
        "  print(\"** {} examples processed\".format(len(input_ids)))\n",
        "\n",
        "  return input_ids, labels, label_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AchPbrDWl4X_"
      },
      "source": [
        "함수를 사용해 train, validation, test 데이터를 각각 처리하겠습니다.   \n",
        "train 데이터를 만들 때는 label_map을 인풋에 주지 않아, 전처리중 label_map이 생성되도록 합니다.   \n",
        "validation과 test 데이터를 만들 때에는 학습 데이터에 대해 만들어진 label_map을 사용해 전처리를 진행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUuJxeHtnTBU",
        "outputId": "a097fe91-f613-4df5-ddfb-b88fcfa553e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# TRAIN\n",
        "train_ids, train_labels, label_map = create_cls_feature(Tokenized_train, text_encoder, max_seq_len=50, label_map = None)\n",
        "# VAL\n",
        "val_ids, val_labels, _ = create_cls_feature(Tokenized_val, text_encoder, max_seq_len=50, label_map = label_map)\n",
        "# TEST\n",
        "test_ids, test_labels, _ = create_cls_feature(Tokenized_test, text_encoder, max_seq_len=50, label_map = label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence with length = 0... continue ['5942978', [], '부정']\n",
            "** 49999 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "Sentence with length = 0... continue ['2172111', [], '긍정']\n",
            "** 9999 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "** 10000 examples processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FRAzerdca6f",
        "outputId": "01d1c0d0-2004-4d20-ce19-6378167c2321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"# Train={} # Val={} # Test={}\".format(len(train_ids), len(val_ids), len(test_ids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Train=49999 # Val=9999 # Test=10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cep29zj6hLjV"
      },
      "source": [
        "모델링을 위한 데이터 전처리가 완료되었습니다!    \n",
        "그럼 데이터를 저장하고, 다음 페이지에서 모델링을 계속 진행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZdu4fphWcS"
      },
      "source": [
        "import pickle\n",
        "prepro_data = {\n",
        "    \"train_ids\": train_ids,\n",
        "    \"train_labels\": train_labels,\n",
        "    \"val_ids\": val_ids,\n",
        "    \"val_labels\": val_labels,\n",
        "    \"test_ids\": test_ids,\n",
        "    \"test_labels\": test_labels,\n",
        "    \"label_map\":label_map\n",
        "}\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_prepro_data.pkl\", \"wb\") as f:\n",
        "  pickle.dump(prepro_data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxUy68PJiIze"
      },
      "source": [
        "utils.py 파일은 드라이브에 저장해, 향후 활용할 수 있도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO_pv12H9YQZ"
      },
      "source": [
        "!cp utils.py \"/content/gdrive/My Drive/NLP/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}